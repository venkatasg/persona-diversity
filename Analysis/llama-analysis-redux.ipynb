{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76ccd819-a164-4766-873d-5235d675a066",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import ipdb\n",
    "import re\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "from diversity import compression_ratio, homogenization_score, ngram_diversity_score, extract_patterns, get_pos, pos_patterns, token_patterns, self_repetition_score\n",
    "import json\n",
    "from collections import Counter\n",
    "from random import shuffle\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# import mplcursors\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "sns.set(style='darkgrid', context='notebook', rc={'figure.figsize':(14,10)}, font_scale=2)\n",
    "\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('chained_assignment',None)\n",
    "\n",
    "# Set random seeds for reproducibility on a specific machine\n",
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "np.random.RandomState(1)\n",
    "np.set_printoptions(precision=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972bcea6-c7a2-48a3-8490-b20ae30a1e83",
   "metadata": {},
   "source": [
    "## Dolly human written responses diversity analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "823907fb-bc10-49b2-a061-2b0f3558c1c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a Spanish-speaking patient with severe myopia interested in LASIK eye surgery'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../data/sample_personas.txt', 'r') as f:\n",
    "    personas = [x.strip() for x in f.readlines()]\n",
    "personas[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a090ecd-41e0-4465-86a7-6f7a81b4c484",
   "metadata": {},
   "outputs": [],
   "source": [
    "dolly = load_dataset(\"databricks/databricks-dolly-15k\")[\"train\"].filter(lambda row: row['category']=='creative_writing').to_pandas()\n",
    "sample = pd.read_csv('../data/dolly_creative_prompts_sample.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2f2128c-c995-4e2f-947a-9d079bdb52bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>instruction</th>\n",
       "      <th>num_tokens</th>\n",
       "      <th>num_tokens_round</th>\n",
       "      <th>num_words_round</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>525</td>\n",
       "      <td>Please propose an argument to convince my mother that she increases the amount of money that I get every month during my studies. I think the current amount is too low.</td>\n",
       "      <td>211</td>\n",
       "      <td>210</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>172</td>\n",
       "      <td>Write a paragraph to refute a claim by a colleague that ancient structures such as Stonehenge, the Great Pyramid are evidence of UFO activities on Earth</td>\n",
       "      <td>189</td>\n",
       "      <td>190</td>\n",
       "      <td>160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>109</td>\n",
       "      <td>What is the best way to answer an interview question?</td>\n",
       "      <td>128</td>\n",
       "      <td>130</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37</td>\n",
       "      <td>Write  the first paragraph of an advertising brochure describing an hotel nearby the Annecy Lake in France</td>\n",
       "      <td>89</td>\n",
       "      <td>90</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>295</td>\n",
       "      <td>Write an intro to a meetup about music, medicine, and machines</td>\n",
       "      <td>313</td>\n",
       "      <td>310</td>\n",
       "      <td>220</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  \\\n",
       "0    525   \n",
       "1    172   \n",
       "2    109   \n",
       "3     37   \n",
       "4    295   \n",
       "\n",
       "                                                                                                                                                                instruction  \\\n",
       "0  Please propose an argument to convince my mother that she increases the amount of money that I get every month during my studies. I think the current amount is too low.   \n",
       "1                  Write a paragraph to refute a claim by a colleague that ancient structures such as Stonehenge, the Great Pyramid are evidence of UFO activities on Earth   \n",
       "2                                                                                                                     What is the best way to answer an interview question?   \n",
       "3                                                                Write  the first paragraph of an advertising brochure describing an hotel nearby the Annecy Lake in France   \n",
       "4                                                                                                            Write an intro to a meetup about music, medicine, and machines   \n",
       "\n",
       "   num_tokens  num_tokens_round  num_words_round  \n",
       "0         211               210              180  \n",
       "1         189               190              160  \n",
       "2         128               130              100  \n",
       "3          89                90               60  \n",
       "4         313               310              220  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "300fd574-c589-45c8-bed1-aa1d2461feda",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>instruction</th>\n",
       "      <th>num_tokens</th>\n",
       "      <th>num_tokens_round</th>\n",
       "      <th>num_words_round</th>\n",
       "      <th>response</th>\n",
       "      <th>prompt_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>525</td>\n",
       "      <td>Please propose an argument to convince my mother that she increases the amount of money that I get every month during my studies. I think the current amount is too low.</td>\n",
       "      <td>211</td>\n",
       "      <td>210</td>\n",
       "      <td>180</td>\n",
       "      <td>Dear Mum, I would like to talk to you about the monthly money I get for my studies. I am very grateful that you support me financially during my studies. At the same time, I feel that the current amount is a little bit too low in order to sustain myself. I estimated my monthly expenses and it seems like the money is not enough. We can go through the details if you want. I want to ask you if you can increase the amount so that I can cover my expenses. I understand if you cannot do this or have other reasons against this. However, I want you to know that if you do not increase my monthly money, I will be forced to work part-time next to my studies. There is research that shows that students who work part-time have worse grades than people who do not have to work part-time to afford their studies. I would love to dedicate my full time to my studies to get the best grades and learning outcomes possible; thus, I would prefer not to work part-time. What do you think?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>172</td>\n",
       "      <td>Write a paragraph to refute a claim by a colleague that ancient structures such as Stonehenge, the Great Pyramid are evidence of UFO activities on Earth</td>\n",
       "      <td>189</td>\n",
       "      <td>190</td>\n",
       "      <td>160</td>\n",
       "      <td>There are several strong arguments against concluding that alien races built the structures like the Stonehenge or the Pyramids. First, there is evidence that these magnificent structures were built by humans. For example, tombs of the pyramid builders were found in the vicinity of the Great Pyramind (with human remains!). The technology to move and assemble the massive stones of the Stonehenge has been shown to be available to humans at that time. Second, given that space is vast the chance of finding one tiny planet among billions is infinitesimal. If alien intelligence had indeed found our one inhabited planet they are unlikely to have just created couple of structures and avoided any further contact with Earth and its inhabitants. In science, Occam’s Razor (“the best explanation is the simplest one”) is a guiding principle and until there is much more evidence of alien visitation it would be inadvisable to jump to a conclusion that aliens built these structures.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>109</td>\n",
       "      <td>What is the best way to answer an interview question?</td>\n",
       "      <td>128</td>\n",
       "      <td>130</td>\n",
       "      <td>100</td>\n",
       "      <td>The first recommended step is to ask clarifying questions to ensure you understand the question correctly and gather the requirements needed to answer effectively. If it is a mathematical or computer science question, go ahead and work your way through the problem while you verbalize your thought process along the way. If it is a situation question, you should use the STAR method. 1. Situation - set the scene and provide context, 2. Task - describe what was needed to be done and why, 3. Action - explain what action you took, and 4. Result - describe the result of your actions and what you learned from the experience</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37</td>\n",
       "      <td>Write  the first paragraph of an advertising brochure describing an hotel nearby the Annecy Lake in France</td>\n",
       "      <td>89</td>\n",
       "      <td>90</td>\n",
       "      <td>60</td>\n",
       "      <td>This fantastic hotel is ideally located, minutes away from the Annecy Lake, one of the purest in Europe. The water temperature reaches 27 degrees celcius at the peak season, allowing the guests to swim and enjoy several water activities including wake surf and pedalo. Finally, this region is known for being the home of several cheese specialties like \"Raclette\" and \"Tartiflette\" to name a few.</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>295</td>\n",
       "      <td>Write an intro to a meetup about music, medicine, and machines</td>\n",
       "      <td>313</td>\n",
       "      <td>310</td>\n",
       "      <td>220</td>\n",
       "      <td>Music, Medicine, and Machines\\n\\nJoin us for an imaginative, authentic, and offbeat networking experience that will advance your knowledge of the technologies that impact the health and well-being of our human condition.  To make things interesting (but keep things real), our meetup incorporates an element of music to remind us, first and foremost, we want to advance technology for the greater good, but we don’t want to become robots ourselves.  Instead, we believe that “music is medicine for the soul” so our events will use a dose of music to unite us and to keep things fun and grounded in our humanity.  We’ll explore hot and emerging technologies such as:\\n\\nMachine Learning and all things AI\\nComputer and Machine Vision\\nTelemedicine\\nGenomics\\nAR/VR/MR \\nRobotics\\nCloud \\nDevOps, CI/CD, and Robotic Process Automation (RPA)\\nInfrastructure as Code (IaC) \\nChatbots\\nWearable Tech\\n3D Printing\\nBlockchain\\nAnd many more\\n\\nWe’ll talk about how these disruptive technologies improve Health &amp; Life Sciences and discuss the tenuous balance of innovation + opportunities vs privacy, security, open data, regulations, etc.  We’ll network and get to know each other to explore how each of us can get involved to ensure “the machines” benefit the communities we serve. We encourage attendees such as developers, clinicians, researchers, industry experts, students, educators, industry analysts, regulators, investors, startups, musicians, and all those willing to contribute meaningfully to our mission.</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  \\\n",
       "0    525   \n",
       "1    172   \n",
       "2    109   \n",
       "3     37   \n",
       "4    295   \n",
       "\n",
       "                                                                                                                                                                instruction  \\\n",
       "0  Please propose an argument to convince my mother that she increases the amount of money that I get every month during my studies. I think the current amount is too low.   \n",
       "1                  Write a paragraph to refute a claim by a colleague that ancient structures such as Stonehenge, the Great Pyramid are evidence of UFO activities on Earth   \n",
       "2                                                                                                                     What is the best way to answer an interview question?   \n",
       "3                                                                Write  the first paragraph of an advertising brochure describing an hotel nearby the Annecy Lake in France   \n",
       "4                                                                                                            Write an intro to a meetup about music, medicine, and machines   \n",
       "\n",
       "   num_tokens  num_tokens_round  num_words_round  \\\n",
       "0         211               210              180   \n",
       "1         189               190              160   \n",
       "2         128               130              100   \n",
       "3          89                90               60   \n",
       "4         313               310              220   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  response  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Dear Mum, I would like to talk to you about the monthly money I get for my studies. I am very grateful that you support me financially during my studies. At the same time, I feel that the current amount is a little bit too low in order to sustain myself. I estimated my monthly expenses and it seems like the money is not enough. We can go through the details if you want. I want to ask you if you can increase the amount so that I can cover my expenses. I understand if you cannot do this or have other reasons against this. However, I want you to know that if you do not increase my monthly money, I will be forced to work part-time next to my studies. There is research that shows that students who work part-time have worse grades than people who do not have to work part-time to afford their studies. I would love to dedicate my full time to my studies to get the best grades and learning outcomes possible; thus, I would prefer not to work part-time. What do you think?   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     There are several strong arguments against concluding that alien races built the structures like the Stonehenge or the Pyramids. First, there is evidence that these magnificent structures were built by humans. For example, tombs of the pyramid builders were found in the vicinity of the Great Pyramind (with human remains!). The technology to move and assemble the massive stones of the Stonehenge has been shown to be available to humans at that time. Second, given that space is vast the chance of finding one tiny planet among billions is infinitesimal. If alien intelligence had indeed found our one inhabited planet they are unlikely to have just created couple of structures and avoided any further contact with Earth and its inhabitants. In science, Occam’s Razor (“the best explanation is the simplest one”) is a guiding principle and until there is much more evidence of alien visitation it would be inadvisable to jump to a conclusion that aliens built these structures.   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          The first recommended step is to ask clarifying questions to ensure you understand the question correctly and gather the requirements needed to answer effectively. If it is a mathematical or computer science question, go ahead and work your way through the problem while you verbalize your thought process along the way. If it is a situation question, you should use the STAR method. 1. Situation - set the scene and provide context, 2. Task - describe what was needed to be done and why, 3. Action - explain what action you took, and 4. Result - describe the result of your actions and what you learned from the experience   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             This fantastic hotel is ideally located, minutes away from the Annecy Lake, one of the purest in Europe. The water temperature reaches 27 degrees celcius at the peak season, allowing the guests to swim and enjoy several water activities including wake surf and pedalo. Finally, this region is known for being the home of several cheese specialties like \"Raclette\" and \"Tartiflette\" to name a few.   \n",
       "4  Music, Medicine, and Machines\\n\\nJoin us for an imaginative, authentic, and offbeat networking experience that will advance your knowledge of the technologies that impact the health and well-being of our human condition.  To make things interesting (but keep things real), our meetup incorporates an element of music to remind us, first and foremost, we want to advance technology for the greater good, but we don’t want to become robots ourselves.  Instead, we believe that “music is medicine for the soul” so our events will use a dose of music to unite us and to keep things fun and grounded in our humanity.  We’ll explore hot and emerging technologies such as:\\n\\nMachine Learning and all things AI\\nComputer and Machine Vision\\nTelemedicine\\nGenomics\\nAR/VR/MR \\nRobotics\\nCloud \\nDevOps, CI/CD, and Robotic Process Automation (RPA)\\nInfrastructure as Code (IaC) \\nChatbots\\nWearable Tech\\n3D Printing\\nBlockchain\\nAnd many more\\n\\nWe’ll talk about how these disruptive technologies improve Health & Life Sciences and discuss the tenuous balance of innovation + opportunities vs privacy, security, open data, regulations, etc.  We’ll network and get to know each other to explore how each of us can get involved to ensure “the machines” benefit the communities we serve. We encourage attendees such as developers, clinicians, researchers, industry experts, students, educators, industry analysts, regulators, investors, startups, musicians, and all those willing to contribute meaningfully to our mission.   \n",
       "\n",
       "   prompt_id  \n",
       "0          0  \n",
       "1          1  \n",
       "2          2  \n",
       "3          3  \n",
       "4          4  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample['response'] = sample['index'].apply(lambda x: dolly.loc[x, 'response'])\n",
    "sample['prompt_id'] = [i for i in range(len(sample))]\n",
    "sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf0d1931-70b6-403b-a8fc-1be1e06701e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = sample['instruction'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb59f56a-59cd-49f4-bcac-a57a48c10f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_responses = sample['response'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88df0513-892d-47a3-aa7a-71c21c0f5354",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.num_tokens_round.sort_values().tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796a2ae7-609b-4cb1-9532-54be158ada3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cr = compression_ratio(human_responses, 'gzip')\n",
    "nds = ngram_diversity_score(human_responses, 4)\n",
    "joined_pos, tuples = get_pos(human_responses)\n",
    "# ngrams_pos = token_patterns(joined_pos, 5, 10)\n",
    "cr_pos = compression_ratio(joined_pos, 'gzip')\n",
    "# rouge = homogenization_score(human_responses, 'rougel', verbose=False)\n",
    "# bleu = homogenization_score(human_responses, 'bleu', verbose=False)\n",
    "srep = self_repetition_score(human_responses)\n",
    "# print(f\"CR: {np.round(cr,2)}\\nNDS: {np.round(nds,2)}\\nCR-POS: {np.round(cr_pos,2)}\\nHS-RougeL: {np.round(rouge,2)}\\nself-bleu: {np.round(bleu,2)}\\nSelf-rep: {np.round(srep, 2)}\")\n",
    "print(f\"CR: {np.round(cr,2)}\\nNDS: {np.round(nds,2)}\\nCR-POS: {np.round(cr_pos,2)}\\nSelf-rep: {np.round(srep, 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93703b3-a60e-4eb1-9605-59e28c4635ee",
   "metadata": {},
   "source": [
    "## Main metric calculation function (can find mean and SD over persona column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b08219b-5931-4b10-bf10-5d91e6168f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_cr_nds_over_personas(df):\n",
    "    crs = []\n",
    "    ndss = []\n",
    "    crs_pos = []\n",
    "    sreps = []\n",
    "    for persona_id in tqdm(df.persona_id.unique()):\n",
    "        responses = df.loc[df.persona_id==persona_id].drop_duplicates(subset=['prompt_id'])['response'].values.tolist()\n",
    "        cr = compression_ratio(responses, 'gzip')\n",
    "        nds = ngram_diversity_score(responses, 4)\n",
    "        #CR-POS\n",
    "        joined_pos, tuples = get_pos(responses)\n",
    "        # ngrams_pos = token_patterns(joined_pos, 5, 10)\n",
    "        cr_pos = compression_ratio(joined_pos, 'gzip')\n",
    "        srep = self_repetition_score(responses, verbose=False)\n",
    "        crs.append(cr)\n",
    "        ndss.append(nds)\n",
    "        crs_pos.append(cr_pos)\n",
    "        sreps.append(srep)\n",
    "    print(f\"CR: {np.round(np.mean(crs),2)} ± {np.round(np.std(crs),2)}\\nCR-POS: {np.round(np.mean(crs_pos),2)} ± {np.round(np.std(crs_pos),2)}\\nNDS: {np.round(np.mean(ndss),2)} ± {np.round(np.std(ndss),2)}\\nSelf-rep:{np.round(np.mean(sreps),2)} ± {np.round(np.std(sreps),2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3175a55a-4934-4d6b-8846-967e2c64e1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_hom_over_personas(df, score='bert'):\n",
    "    homs = []\n",
    "    for persona_id in tqdm(df.persona_id.unique()):\n",
    "        responses = df.loc[df.persona_id==persona_id].drop_duplicates(subset=['prompt_id'])['response'].values.tolist()\n",
    "        \n",
    "        hom = homogenization_score(responses, 'bertscore', verbose=False)\n",
    "        homs.append(hom)\n",
    "        \n",
    "    print(f\"Hom-{score}: {np.round(np.mean(homs),2)} ± {np.round(np.std(homs),2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d099f3-8a7a-4803-a93c-2f697148b032",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(homogenization_score(human_responses, 'bertscore', verbose=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc7ed22-8103-4138-9ad6-4dd1008f50ae",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Llama-8B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c89dd4-2f25-4525-a534-e6f5688e72a2",
   "metadata": {},
   "source": [
    "## No persona, no cutoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d58ea48-9540-4da3-98bd-aaeb0a7ad9d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np_df = pd.read_csv('../output/llama-temp0.7/llama8b-np/Llama-3.1-8B-Instruct_dolly_output.tsv', sep='\\t')\n",
    "np_df['response'] = np_df.response.apply(lambda x: x.strip())\n",
    "np_df['prompt_id'] = [i for i in range(len(prompts))]\n",
    "np_df['prompt'] = np_df.prompt_id.apply(lambda x: prompts[x])\n",
    "np_df['persona_id'] = [-1 for i in range(len(prompts))]\n",
    "np_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf28ab21-593c-4de3-ae62-54157481f9ac",
   "metadata": {},
   "source": [
    "## Figure out the language of response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0945a752-cf69-4b25-8fbe-e98aba184b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "model_path = hf_hub_download(repo_id=\"facebook/fasttext-language-identification\", filename=\"model.bin\")\n",
    "model = fasttext.load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a16363-0cfd-4011-a530-15a823ad4d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_df['lang'] = np_df.response.progress_apply(lambda x: model.predict(x.replace('\\n', ' '))[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61192151-e399-4d5f-b9c9-fbd7496fa3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(np_df.lang.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07298219-f207-4129-bd36-3150a7607a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_cr_nds_over_personas(np_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5eca4e-0169-4676-8398-89898c4e8ab8",
   "metadata": {},
   "source": [
    "## No persona with cutoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dab477d-db2c-4a43-b63a-30974050c3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "npc_df = pd.read_csv('../output/llama8b-cutoff-np/Llama-3.1-8B-Instruct_dolly_output.tsv', sep='\\t')\n",
    "npc_df['response'] = npc_df.response.apply(lambda x: x.strip())\n",
    "npc_df['prompt_id'] = [i for i in range(len(prompts))]\n",
    "npc_df['prompt'] = npc_df.prompt_id.apply(lambda x: prompts[x])\n",
    "npc_df['persona_id'] = [-1 for i in range(len(prompts))]\n",
    "\n",
    "npc_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d069d93c-6c00-40d5-a8c1-80f1f691933d",
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_cr_nds_over_personas(npc_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3576720-3a9b-4eda-bf3e-c7b0fbe1de43",
   "metadata": {},
   "source": [
    "## Persona, no cutoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d55879-2cb3-4a5c-94ce-e93feaeccc9d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "persona_df = pd.read_csv('../output/llama8b-persona/Llama-3.1-8B-Instruct_dolly_output.tsv', sep='\\t')\n",
    "persona_df['response'] = persona_df.response.apply(lambda x: x.strip())\n",
    "\n",
    "persona_ids = []\n",
    "prompt_ids = []\n",
    "for persona_id in range(100):\n",
    "    for prompt_id in range(100):\n",
    "        persona_ids += [persona_id]\n",
    "        prompt_ids += [prompt_id]\n",
    "if 'persona_id' not in persona_df.columns:\n",
    "    persona_df['prompt_id'] = prompt_ids\n",
    "    persona_df['prompt'] = persona_df.prompt_id.apply(lambda x: prompts[x])\n",
    "    persona_df['persona_id'] = persona_ids\n",
    "    persona_df['persona'] = persona_df.persona_id.apply(lambda x: personas[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1eaad3a-71ce-48e5-9ff7-b482c20041bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_cr_nds_over_personas(persona_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e5e308-e078-4fc9-8130-3c2de684aa6f",
   "metadata": {},
   "source": [
    "### Now to check the lang of responses and get metrics with non-english removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ddc259-8c15-4199-ab2c-d35a2669fd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "persona_df['lang'] = persona_df.response.progress_apply(lambda x: model.predict(x.replace('\\n', ' '))[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e45e42-a4f5-4f22-b2ab-0c5899e71bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in persona_df[persona_df.lang!='__label__eng_Latn']['persona_id'].unique():\n",
    "    print(p,personas[p])\n",
    "\n",
    "print(persona_df[persona_df.lang!='__label__eng_Latn']['persona_id'].unique().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b64edb-d2df-468d-a220-ff8a9d56b638",
   "metadata": {},
   "outputs": [],
   "source": [
    "noneng_counts=Counter(persona_df[persona_df.lang!='__label__eng_Latn']['persona_id'].values.tolist())\n",
    "print(noneng_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69abebab-52cf-486e-bacd-c5c08b78e681",
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_noneng_personas = [x for x in noneng_counts if noneng_counts[x]>2]\n",
    "print(possible_noneng_personas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac8194e-394d-4579-846c-c6a7b135360d",
   "metadata": {},
   "source": [
    "First just try all prompt responses labelled as english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0ad3bc-5cbe-4510-b19d-9231935d8deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_persona_df = persona_df[persona_df.lang=='__label__eng_Latn']\n",
    "print(eng_persona_df.shape)\n",
    "calc_cr_nds_over_personas(eng_persona_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff282b6-267e-4005-9ed6-e0ecf45083b4",
   "metadata": {},
   "source": [
    "Now to try with all non-english personas possibly removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0e4b59-c3aa-4eaf-b02e-c04c0b987f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_persona_df2 = persona_df.loc[~persona_df.persona_id.isin(possible_noneng_personas)]\n",
    "print(eng_persona_df2.shape)\n",
    "calc_cr_nds_over_personas(eng_persona_df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ef4a50-18fd-49e2-8629-acdb95ea5b17",
   "metadata": {},
   "source": [
    "## Persona plus cutoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de13de32-97f9-4cf0-95b2-5dc4b20de356",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "personac_df = pd.read_csv('../output/llama8b-cutoff-persona/Llama-3.1-8B-Instruct_dolly_output.tsv', sep='\\t')\n",
    "personac_df['response'] = personac_df.response.apply(lambda x: x.strip())\n",
    "\n",
    "persona_ids = []\n",
    "prompt_ids = []\n",
    "for persona_id in range(100):\n",
    "    for prompt_id in range(100):\n",
    "        persona_ids += [persona_id]\n",
    "        prompt_ids += [prompt_id]\n",
    "if 'persona_id' not in personac_df.columns:\n",
    "    personac_df['prompt_id'] = prompt_ids\n",
    "    personac_df['prompt'] = personac_df.prompt_id.apply(lambda x: prompts[x])\n",
    "    personac_df['persona_id'] = persona_ids\n",
    "    personac_df['persona'] = personac_df.persona_id.apply(lambda x: personas[x])\n",
    "\n",
    "personac_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bb9f91-7e6a-4201-938e-e2ca381f59c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_cr_nds_over_personas(personac_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bf445c-b740-4b42-a511-32a59f7aa2fe",
   "metadata": {},
   "source": [
    "### Now to check the lang of responses and get metrics with non-english removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0973910a-5c36-4661-b849-5c1d12d05a3c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "personac_df['lang'] = personac_df.response.progress_apply(lambda x: model.predict(x.replace('\\n', ' '))[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7734606-7b71-473e-8d5b-78974653088f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in personac_df[personac_df.lang!='__label__eng_Latn']['persona_id'].unique():\n",
    "    print(p,personas[p])\n",
    "\n",
    "print(personac_df[personac_df.lang!='__label__eng_Latn']['persona_id'].unique().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc8c0c6-48e0-4e31-b440-849601ceee1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "noneng_counts=Counter(personac_df[personac_df.lang!='__label__eng_Latn']['persona_id'].values.tolist())\n",
    "print(noneng_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d878c9d-f822-42fe-b356-7549b41e28a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_noneng_personas = [x for x in noneng_counts if noneng_counts[x]>2]\n",
    "print(possible_noneng_personas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6c0a1a-8504-4a5c-ab54-a0cd1a2db6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "personac_df[personac_df.lang!='__label__eng_Latn']['prompt_id'].unique().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0fe684-9b0a-4058-99ab-56b5cb4309cd",
   "metadata": {},
   "source": [
    "First just try all prompt responses labelled as english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea46771-3313-4a86-8de2-853b8e5e22d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_personac_df = personac_df[personac_df.lang=='__label__eng_Latn']\n",
    "print(eng_personac_df.shape)\n",
    "calc_cr_nds_over_personas(eng_personac_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36eb16c8-5bc6-44e3-853f-1e31401f85ea",
   "metadata": {},
   "source": [
    "Now to try with all possibly non-english personas removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cf1379-8405-4ae1-9707-e8d8a593c3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_personac_df2 = personac_df.loc[~personac_df.persona_id.isin(possible_noneng_personas)]\n",
    "print(eng_personac_df2.shape)\n",
    "calc_cr_nds_over_personas(eng_personac_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0100080-246e-4749-9303-e2b855546392",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "personac_df[personac_df.lang!='__label__eng_Latn'].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b84f26b-5ae8-45c2-b639-12473a93b1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "personac_df.loc[((personac_df.persona_id==99) & (personac_df.lang!='__label__eng_Latn'))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50133bdf-f03a-43eb-a3e1-7697782fbc0c",
   "metadata": {},
   "source": [
    "# Llama-70B temp=0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e8fe933d-fcbf-4634-8468-e92bde2e6d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:06<00:00,  6.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CR: 2.76 ± 0.0\n",
      "CR-POS: 5.72 ± 0.0\n",
      "NDS: 2.87 ± 0.0\n",
      "Self-rep:1.96 ± 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# No persona, no cutoff\n",
    "np70_df = pd.read_csv('../output/llama-temp0.7/llama70b-np/Llama-3.3-70B-Instruct-Turbo_dolly_output.tsv', sep='\\t')\n",
    "np70_df['response'] = np70_df.response.apply(lambda x: x.strip())\n",
    "np70_df['prompt_id'] = [i for i in range(len(prompts))]\n",
    "np70_df['persona_id'] = [-1 for i in range(len(prompts))]\n",
    "calc_cr_nds_over_personas(np70_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2e03904f-38e5-46e9-b2f4-3c0f2dfc687a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:05<00:00,  5.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CR: 2.57 ± 0.0\n",
      "CR-POS: 5.1 ± 0.0\n",
      "NDS: 3.1 ± 0.0\n",
      "Self-rep:0.44 ± 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# No persona with cutoff\n",
    "npc70_df = pd.read_csv('../output/llama-temp0.7/llama70b-cutoff-np/Llama-3.3-70B-Instruct-Turbo_dolly_output.tsv', sep='\\t')\n",
    "npc70_df['response'] = npc70_df.response.apply(lambda x: x.strip())\n",
    "npc70_df['prompt_id'] = [i for i in range(len(prompts))]\n",
    "npc70_df['persona_id'] = [-1 for i in range(len(prompts))]\n",
    "calc_cr_nds_over_personas(npc70_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ebedb7d6-95c8-4493-9e2b-c4c7309cd686",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>persona_id</th>\n",
       "      <th>response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>As a PR manager, I'd frame your request as a compelling narrative. Emphasize your academic goals and how increased financial support will enable you to focus on your studies, leading to better grades and a brighter future. Highlight specific expenses, such as textbooks, transportation, or extracurricular activities, that the current amount doesn't cover. \\n\\nYou could say, \"Mom, I understand that you want me to be responsible with money, but I believe that with a bit more support, I can achieve my academic goals and make you proud. I've done some research and found that my expenses are higher than what I currently receive. I'd like to discuss a possible increase to ensure I can cover all my necessary expenses and focus on my studies without added stress.\" This approach shows that you've thoughtfully considered your needs and are committed to your education.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>I understand where my colleague is coming from, but as a PR manager, I've seen how misinformation can spread like wildfire. The notion that ancient structures like Stonehenge and the Great Pyramid are evidence of UFO activities is a fascinating narrative, but it's essential to separate fact from fiction. These iconic landmarks have been extensively studied by archaeologists, historians, and scientists, who attribute their construction to the ingenuity and skill of our ancient ancestors. While it's intriguing to consider alternative explanations, there's no credible evidence to support the claim of UFO involvement. Let's focus on promoting accurate information and respecting the cultural heritage of these incredible achievements.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>As a PR manager, I advise influencers to use the STAR method: Situation, Task, Action, Result. This framework helps them structure their response, providing context, highlighting their actions, and showcasing the outcome, making their answer clear, concise, and engaging for the audience.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   prompt_id  persona_id  \\\n",
       "0          0           0   \n",
       "1          1           0   \n",
       "2          2           0   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                response  \n",
       "0  As a PR manager, I'd frame your request as a compelling narrative. Emphasize your academic goals and how increased financial support will enable you to focus on your studies, leading to better grades and a brighter future. Highlight specific expenses, such as textbooks, transportation, or extracurricular activities, that the current amount doesn't cover. \\n\\nYou could say, \"Mom, I understand that you want me to be responsible with money, but I believe that with a bit more support, I can achieve my academic goals and make you proud. I've done some research and found that my expenses are higher than what I currently receive. I'd like to discuss a possible increase to ensure I can cover all my necessary expenses and focus on my studies without added stress.\" This approach shows that you've thoughtfully considered your needs and are committed to your education.  \n",
       "1                                                                                                                                     I understand where my colleague is coming from, but as a PR manager, I've seen how misinformation can spread like wildfire. The notion that ancient structures like Stonehenge and the Great Pyramid are evidence of UFO activities is a fascinating narrative, but it's essential to separate fact from fiction. These iconic landmarks have been extensively studied by archaeologists, historians, and scientists, who attribute their construction to the ingenuity and skill of our ancient ancestors. While it's intriguing to consider alternative explanations, there's no credible evidence to support the claim of UFO involvement. Let's focus on promoting accurate information and respecting the cultural heritage of these incredible achievements.  \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       As a PR manager, I advise influencers to use the STAR method: Situation, Task, Action, Result. This framework helps them structure their response, providing context, highlighting their actions, and showcasing the outcome, making their answer clear, concise, and engaging for the audience.  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Persona plus cutoff\n",
    "personac70_df = pd.read_csv('../output/llama-temp0.7/llama70b-cutoff-persona/Llama-3.3-70B-Instruct-Turbo_dolly_output.tsv', sep='\\t')\n",
    "personac70_df['response'] = personac70_df.response.apply(lambda x: x.strip())\n",
    "\n",
    "if 'persona_id' not in personac70_df.columns:\n",
    "    print(\"Adding persona prompt ids\")\n",
    "    persona_ids = []\n",
    "    prompt_ids = []\n",
    "    for persona_id in range(100):\n",
    "        for prompt_id in range(100):\n",
    "            persona_ids += [persona_id]\n",
    "            prompt_ids += [prompt_id]\n",
    "    personac70_df['prompt_id'] = prompt_ids\n",
    "    personac70_df['prompt'] = personac70_df.prompt_id.apply(lambda x: prompts[x])\n",
    "    personac70_df['persona_id'] = persona_ids\n",
    "    personac70_df['persona'] = personac70_df.persona_id.apply(lambda x: personas[x])\n",
    "\n",
    "personac70_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc1193f9-72d9-4f27-b410-85344e6ba1d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|█▉                                                                                             | 2/100 [00:13<10:39,  6.52s/it]\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/multiprocessing/spawn.py\", line 120, in spawn_main\n",
      "Process Process-9:\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/multiprocessing/spawn.py\", line 130, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/site-packages/spacy/__init__.py\", line 6, in <module>\n",
      "    from .errors import setup_default_warnings\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/site-packages/spacy/errors.py\", line 3, in <module>\n",
      "    from .compat import Literal\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/site-packages/spacy/compat.py\", line 4, in <module>\n",
      "    from thinc.util import copy_array\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/site-packages/thinc/__init__.py\", line 5, in <module>\n",
      "    from .config import registry\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/site-packages/thinc/config.py\", line 5, in <module>\n",
      "    from .types import Decorator\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/site-packages/thinc/types.py\", line 25, in <module>\n",
      "    from .compat import cupy, has_cupy\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/site-packages/thinc/compat.py\", line 35, in <module>\n",
      "    import torch\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/site-packages/torch/__init__.py\", line 2046, in <module>\n",
      "    _C._initExtension(_manager_path())\n",
      "  File \"<frozen importlib._bootstrap>\", line 224, in _lock_unlock_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 110, in acquire\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/site-packages/spacy/language.py\", line 2409, in _apply_pipes\n",
      "    byte_docs = [(doc.to_bytes(), doc._context, None) for doc in docs]\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/site-packages/spacy/language.py\", line 2409, in <listcomp>\n",
      "    byte_docs = [(doc.to_bytes(), doc._context, None) for doc in docs]\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/site-packages/spacy/util.py\", line 1703, in _pipe\n",
      "    yield from proc.pipe(docs, **kwargs)\n",
      "  File \"spacy/pipeline/trainable_pipe.pyx\", line 75, in pipe\n",
      "  File \"spacy/pipeline/tagger.pyx\", line 138, in spacy.pipeline.tagger.Tagger.predict\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/site-packages/thinc/model.py\", line 334, in predict\n",
      "    return self._func(self, X, is_train=False)[0]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/site-packages/thinc/layers/chain.py\", line 54, in forward\n",
      "    Y, inc_layer_grad = layer(X, is_train=is_train)\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/site-packages/thinc/model.py\", line 310, in __call__\n",
      "    return self._func(self, X, is_train=is_train)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/site-packages/thinc/layers/with_array.py\", line 42, in forward\n",
      "    return cast(Tuple[SeqT, Callable], _list_forward(model, Xseq, is_train))\n",
      "                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/site-packages/thinc/layers/with_array.py\", line 77, in _list_forward\n",
      "    Yf, get_dXf = layer(Xf, is_train)\n",
      "                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/site-packages/thinc/model.py\", line 310, in __call__\n",
      "    return self._func(self, X, is_train=is_train)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/site-packages/thinc/layers/softmax.py\", line 71, in forward\n",
      "    Y = model.ops.affine(X, W, b)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/site-packages/thinc/backends/ops.py\", line 263, in affine\n",
      "    Y = self.gemm(X, W, trans2=True)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m calc_cr_nds_over_personas(personac70_df)\n",
      "Cell \u001b[0;32mIn[8], line 11\u001b[0m, in \u001b[0;36mcalc_cr_nds_over_personas\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m      9\u001b[0m nds \u001b[38;5;241m=\u001b[39m ngram_diversity_score(responses, \u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#CR-POS\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m joined_pos, tuples \u001b[38;5;241m=\u001b[39m get_pos(responses)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# ngrams_pos = token_patterns(joined_pos, 5, 10)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m cr_pos \u001b[38;5;241m=\u001b[39m compression_ratio(joined_pos, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgzip\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/micromamba/envs/diversity/lib/python3.11/site-packages/diversity/patterns/part_of_speech.py:24\u001b[0m, in \u001b[0;36mget_pos\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     20\u001b[0m joined_text \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     22\u001b[0m docs \u001b[38;5;241m=\u001b[39m nlp\u001b[38;5;241m.\u001b[39mpipe(data, n_process\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m docs:\n\u001b[1;32m     25\u001b[0m     joined_text\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([token\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m doc]))\n\u001b[1;32m     26\u001b[0m     joined_pos\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([token\u001b[38;5;241m.\u001b[39mtag_ \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m doc]))\n",
      "File \u001b[0;32m~/micromamba/envs/diversity/lib/python3.11/site-packages/spacy/language.py:1621\u001b[0m, in \u001b[0;36mLanguage.pipe\u001b[0;34m(self, texts, as_tuples, batch_size, disable, component_cfg, n_process)\u001b[0m\n\u001b[1;32m   1619\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m pipe \u001b[38;5;129;01min\u001b[39;00m pipes:\n\u001b[1;32m   1620\u001b[0m         docs \u001b[38;5;241m=\u001b[39m pipe(docs)\n\u001b[0;32m-> 1621\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m docs:\n\u001b[1;32m   1622\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m doc\n",
      "File \u001b[0;32m~/micromamba/envs/diversity/lib/python3.11/site-packages/spacy/language.py:1687\u001b[0m, in \u001b[0;36mLanguage._multiprocessing_pipe\u001b[0;34m(self, texts, pipes, n_process, batch_size)\u001b[0m\n\u001b[1;32m   1673\u001b[0m procs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   1674\u001b[0m     mp\u001b[38;5;241m.\u001b[39mProcess(\n\u001b[1;32m   1675\u001b[0m         target\u001b[38;5;241m=\u001b[39m_apply_pipes,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1684\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m rch, sch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(texts_q, bytedocs_send_ch)\n\u001b[1;32m   1685\u001b[0m ]\n\u001b[1;32m   1686\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m proc \u001b[38;5;129;01min\u001b[39;00m procs:\n\u001b[0;32m-> 1687\u001b[0m     proc\u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m   1689\u001b[0m \u001b[38;5;66;03m# Close writing-end of channels. This is needed to avoid that reading\u001b[39;00m\n\u001b[1;32m   1690\u001b[0m \u001b[38;5;66;03m# from the channel blocks indefinitely when the worker closes the\u001b[39;00m\n\u001b[1;32m   1691\u001b[0m \u001b[38;5;66;03m# channel.\u001b[39;00m\n\u001b[1;32m   1692\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tx \u001b[38;5;129;01min\u001b[39;00m bytedocs_send_ch:\n",
      "File \u001b[0;32m~/micromamba/envs/diversity/lib/python3.11/multiprocessing/process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[1;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    120\u001b[0m _cleanup()\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Popen(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[0;32m~/micromamba/envs/diversity/lib/python3.11/multiprocessing/context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[0;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _default_context\u001b[38;5;241m.\u001b[39mget_context()\u001b[38;5;241m.\u001b[39mProcess\u001b[38;5;241m.\u001b[39m_Popen(process_obj)\n",
      "File \u001b[0;32m~/micromamba/envs/diversity/lib/python3.11/multiprocessing/context.py:288\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_spawn_posix\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[0;32m--> 288\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Popen(process_obj)\n",
      "File \u001b[0;32m~/micromamba/envs/diversity/lib/python3.11/multiprocessing/popen_spawn_posix.py:32\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, process_obj):\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fds \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(process_obj)\n",
      "File \u001b[0;32m~/micromamba/envs/diversity/lib/python3.11/multiprocessing/popen_fork.py:19\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinalizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_launch(process_obj)\n",
      "File \u001b[0;32m~/micromamba/envs/diversity/lib/python3.11/multiprocessing/popen_spawn_posix.py:62\u001b[0m, in \u001b[0;36mPopen._launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msentinel \u001b[38;5;241m=\u001b[39m parent_r\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(parent_w, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m, closefd\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 62\u001b[0m         f\u001b[38;5;241m.\u001b[39mwrite(fp\u001b[38;5;241m.\u001b[39mgetbuffer())\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m     fds_to_close \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "calc_cr_nds_over_personas(personac70_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d81a45-4672-46ab-9d16-af5eef9b3293",
   "metadata": {},
   "outputs": [],
   "source": [
    "personac70_df['lang'] = personac70_df.response.progress_apply(lambda x: model.predict(x.replace('\\n', ' '))[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d965396-d844-4477-973c-f1a6786b03d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in personac70_df[personac70_df.lang!='__label__eng_Latn']['persona_id'].unique():\n",
    "    print(p,personas[p])\n",
    "\n",
    "print(personac70_df[personac70_df.lang!='__label__eng_Latn']['persona_id'].unique().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3daa5f28-14c9-4428-9252-20210a38c00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "noneng_counts=Counter(personac70_df[personac70_df.lang!='__label__eng_Latn']['persona_id'].values.tolist())\n",
    "print(noneng_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b058dd4-8f1c-4761-902b-4820b3066554",
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_noneng_personas = [x for x in noneng_counts if noneng_counts[x]>2]\n",
    "print(possible_noneng_personas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a381c06-67a4-40db-9c5e-b9cbcf85614f",
   "metadata": {},
   "outputs": [],
   "source": [
    "personac70_df[personac70_df.lang!='__label__eng_Latn']['prompt_id'].unique().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408ee5e6-f29a-4811-97e6-72da6c773ecf",
   "metadata": {},
   "source": [
    "First just try all prompt responses labelled as english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26459c8-f644-40f3-8537-bb04f0bc30fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_personac70_df = personac70_df[personac70_df.lang=='__label__eng_Latn']\n",
    "print(eng_personac70_df.shape)\n",
    "calc_cr_nds_over_personas(eng_personac70_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3fe27f-eb4f-47e7-8342-8ebef5b4c628",
   "metadata": {},
   "source": [
    "Now to try with all possibly non-english personas removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576c4e2f-bd0b-406f-8264-7e4af81b3543",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_personac70_df2 = personac70_df.loc[~personac70_df.persona_id.isin(possible_noneng_personas)]\n",
    "print(eng_personac70_df2.shape)\n",
    "calc_cr_nds_over_personas(eng_personac70_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f2a50f59-1cbf-486b-9ec2-5d4291ea07fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 3)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Persona no cutoff\n",
    "persona70_df = pd.read_csv('../output/llama-temp0.7/llama70b-persona/Llama-3.3-70B-Instruct-Turbo_dolly_output.tsv', sep='\\t')\n",
    "persona70_df['response'] = persona70_df.response.apply(lambda x: x.strip())\n",
    "\n",
    "if 'persona_id' not in persona70_df.columns:\n",
    "    print(\"Adding persona prompt ids\")\n",
    "    persona_ids = []\n",
    "    prompt_ids = []\n",
    "    for persona_id in range(100):\n",
    "        for prompt_id in range(100):\n",
    "            persona_ids += [persona_id]\n",
    "            prompt_ids += [prompt_id]\n",
    "    persona70_df['prompt_id'] = prompt_ids\n",
    "    persona70_df['prompt'] = persona70_df.prompt_id.apply(lambda x: prompts[x])\n",
    "    persona70_df['persona_id'] = persona_ids\n",
    "    persona70_df['persona'] = persona70_df.persona_id.apply(lambda x: personas[x])\n",
    "\n",
    "persona70_df = persona70_df.drop_duplicates(subset=['prompt_id', 'persona_id'], keep='first')\n",
    "persona70_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7c753be1-e5d8-4601-bbb3-82dadd3b9464",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-104:                                                                                        | 0/100 [00:00<?, ?it/s]\n",
      "Process Process-101:\n",
      "Process Process-102:\n",
      "Process Process-103:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/site-packages/spacy/language.py\", line 2395, in _apply_pipes\n",
      "    texts_with_ctx = receiver.get()\n",
      "                     ^^^^^^^^^^^^^^\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/site-packages/spacy/language.py\", line 2395, in _apply_pipes\n",
      "    texts_with_ctx = receiver.get()\n",
      "                     ^^^^^^^^^^^^^^\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/multiprocessing/queues.py\", line 103, in get\n",
      "    res = self._recv_bytes()\n",
      "          ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/multiprocessing/queues.py\", line 103, in get\n",
      "    res = self._recv_bytes()\n",
      "          ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/multiprocessing/connection.py\", line 215, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/multiprocessing/connection.py\", line 215, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/site-packages/spacy/language.py\", line 2395, in _apply_pipes\n",
      "    texts_with_ctx = receiver.get()\n",
      "                     ^^^^^^^^^^^^^^\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/multiprocessing/connection.py\", line 413, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "          ^^^^^^^^^^^^^\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/multiprocessing/connection.py\", line 413, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "          ^^^^^^^^^^^^^\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/multiprocessing/queues.py\", line 103, in get\n",
      "    res = self._recv_bytes()\n",
      "          ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/multiprocessing/connection.py\", line 215, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/multiprocessing/connection.py\", line 378, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/multiprocessing/connection.py\", line 413, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "          ^^^^^^^^^^^^^\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/site-packages/spacy/language.py\", line 2395, in _apply_pipes\n",
      "    texts_with_ctx = receiver.get()\n",
      "                     ^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/multiprocessing/connection.py\", line 378, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/multiprocessing/connection.py\", line 378, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/multiprocessing/queues.py\", line 103, in get\n",
      "    res = self._recv_bytes()\n",
      "          ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/multiprocessing/connection.py\", line 215, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/multiprocessing/connection.py\", line 413, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "          ^^^^^^^^^^^^^\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/multiprocessing/connection.py\", line 378, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "/Users/venkat/micromamba/envs/diversity/lib/python3.11/site-packages/spacy/language.py:1737: UserWarning: [W127] Not all `Language.pipe` worker processes completed successfully\n",
      "  warnings.warn(Warnings.W127)\n",
      "  0%|                                                                                                       | 0/100 [00:06<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m calc_cr_nds_over_personas(persona70_df)\n",
      "Cell \u001b[0;32mIn[8], line 11\u001b[0m, in \u001b[0;36mcalc_cr_nds_over_personas\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m      9\u001b[0m nds \u001b[38;5;241m=\u001b[39m ngram_diversity_score(responses, \u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#CR-POS\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m joined_pos, tuples \u001b[38;5;241m=\u001b[39m get_pos(responses)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# ngrams_pos = token_patterns(joined_pos, 5, 10)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m cr_pos \u001b[38;5;241m=\u001b[39m compression_ratio(joined_pos, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgzip\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/micromamba/envs/diversity/lib/python3.11/site-packages/diversity/patterns/part_of_speech.py:24\u001b[0m, in \u001b[0;36mget_pos\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     20\u001b[0m joined_text \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     22\u001b[0m docs \u001b[38;5;241m=\u001b[39m nlp\u001b[38;5;241m.\u001b[39mpipe(data, n_process\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m docs:\n\u001b[1;32m     25\u001b[0m     joined_text\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([token\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m doc]))\n\u001b[1;32m     26\u001b[0m     joined_pos\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([token\u001b[38;5;241m.\u001b[39mtag_ \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m doc]))\n",
      "File \u001b[0;32m~/micromamba/envs/diversity/lib/python3.11/site-packages/spacy/language.py:1621\u001b[0m, in \u001b[0;36mLanguage.pipe\u001b[0;34m(self, texts, as_tuples, batch_size, disable, component_cfg, n_process)\u001b[0m\n\u001b[1;32m   1619\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m pipe \u001b[38;5;129;01min\u001b[39;00m pipes:\n\u001b[1;32m   1620\u001b[0m         docs \u001b[38;5;241m=\u001b[39m pipe(docs)\n\u001b[0;32m-> 1621\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m docs:\n\u001b[1;32m   1622\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m doc\n",
      "File \u001b[0;32m~/micromamba/envs/diversity/lib/python3.11/site-packages/spacy/language.py:1705\u001b[0m, in \u001b[0;36mLanguage._multiprocessing_pipe\u001b[0;34m(self, texts, pipes, n_process, batch_size)\u001b[0m\n\u001b[1;32m   1701\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (_, (byte_doc, context, byte_error)) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\n\u001b[1;32m   1702\u001b[0m     \u001b[38;5;28mzip\u001b[39m(raw_texts, byte_tuples), \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1703\u001b[0m ):\n\u001b[1;32m   1704\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m byte_doc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1705\u001b[0m         doc \u001b[38;5;241m=\u001b[39m Doc(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab)\u001b[38;5;241m.\u001b[39mfrom_bytes(byte_doc)\n\u001b[1;32m   1706\u001b[0m         doc\u001b[38;5;241m.\u001b[39m_context \u001b[38;5;241m=\u001b[39m context\n\u001b[1;32m   1707\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m doc\n",
      "File \u001b[0;32m~/micromamba/envs/diversity/lib/python3.11/site-packages/spacy/tokens/doc.pyx:1362\u001b[0m, in \u001b[0;36mspacy.tokens.doc.Doc.from_bytes\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/micromamba/envs/diversity/lib/python3.11/site-packages/srsly/_msgpack_api.py:27\u001b[0m, in \u001b[0;36mmsgpack_loads\u001b[0;34m(data, use_list)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# msgpack-python docs suggest disabling gc before unpacking large messages\u001b[39;00m\n\u001b[1;32m     26\u001b[0m gc\u001b[38;5;241m.\u001b[39mdisable()\n\u001b[0;32m---> 27\u001b[0m msg \u001b[38;5;241m=\u001b[39m msgpack\u001b[38;5;241m.\u001b[39mloads(data, raw\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, use_list\u001b[38;5;241m=\u001b[39muse_list)\n\u001b[1;32m     28\u001b[0m gc\u001b[38;5;241m.\u001b[39menable()\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m msg\n",
      "File \u001b[0;32m~/micromamba/envs/diversity/lib/python3.11/site-packages/srsly/msgpack/__init__.py:82\u001b[0m, in \u001b[0;36munpackb\u001b[0;34m(packed, **kwargs)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject_pairs_hook\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[1;32m     81\u001b[0m     object_hook \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject_hook\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 82\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m decoder \u001b[38;5;129;01min\u001b[39;00m msgpack_decoders\u001b[38;5;241m.\u001b[39mget_all()\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[1;32m     83\u001b[0m         object_hook \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(decoder, chain\u001b[38;5;241m=\u001b[39mobject_hook)\n\u001b[1;32m     84\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject_hook\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m object_hook\n",
      "File \u001b[0;32m~/micromamba/envs/diversity/lib/python3.11/site-packages/catalogue/__init__.py:110\u001b[0m, in \u001b[0;36mRegistry.get_all\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    108\u001b[0m result \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mentry_points:\n\u001b[0;32m--> 110\u001b[0m     result\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_entry_points())\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m keys, value \u001b[38;5;129;01min\u001b[39;00m REGISTRY\u001b[38;5;241m.\u001b[39mcopy()\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnamespace) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(keys) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\n\u001b[1;32m    113\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnamespace[i] \u001b[38;5;241m==\u001b[39m keys[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnamespace))\n\u001b[1;32m    114\u001b[0m     ):\n",
      "File \u001b[0;32m~/micromamba/envs/diversity/lib/python3.11/site-packages/catalogue/__init__.py:124\u001b[0m, in \u001b[0;36mRegistry.get_entry_points\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get registered entry points from other packages for this namespace.\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \n\u001b[1;32m    121\u001b[0m \u001b[38;5;124;03mRETURNS (Dict[str, Any]): Entry points, keyed by name.\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    123\u001b[0m result \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 124\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m entry_point \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_entry_points():\n\u001b[1;32m    125\u001b[0m     result[entry_point\u001b[38;5;241m.\u001b[39mname] \u001b[38;5;241m=\u001b[39m entry_point\u001b[38;5;241m.\u001b[39mload()\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/micromamba/envs/diversity/lib/python3.11/site-packages/catalogue/__init__.py:143\u001b[0m, in \u001b[0;36mRegistry._get_entry_points\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_entry_points\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[importlib_metadata\u001b[38;5;241m.\u001b[39mEntryPoint]:\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(AVAILABLE_ENTRY_POINTS, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mselect\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 143\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m AVAILABLE_ENTRY_POINTS\u001b[38;5;241m.\u001b[39mselect(group\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mentry_point_namespace)\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# dict\u001b[39;00m\n\u001b[1;32m    145\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m AVAILABLE_ENTRY_POINTS\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mentry_point_namespace, [])\n",
      "File \u001b[0;32m~/micromamba/envs/diversity/lib/python3.11/importlib/metadata/__init__.py:500\u001b[0m, in \u001b[0;36mSelectableGroups.select\u001b[0;34m(self, **params)\u001b[0m\n\u001b[1;32m    498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m params:\n\u001b[1;32m    499\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[0;32m--> 500\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_all\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n",
      "File \u001b[0;32m~/micromamba/envs/diversity/lib/python3.11/importlib/metadata/__init__.py:376\u001b[0m, in \u001b[0;36mEntryPoints.select\u001b[0;34m(self, **params)\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams):\n\u001b[1;32m    372\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;124;03m    Select entry points from self that match the\u001b[39;00m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;124;03m    given parameters (typically group and/or name).\u001b[39;00m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 376\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m EntryPoints(ep \u001b[38;5;28;01mfor\u001b[39;00m ep \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ep\u001b[38;5;241m.\u001b[39mmatches(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams))\n",
      "File \u001b[0;32m~/micromamba/envs/diversity/lib/python3.11/importlib/metadata/__init__.py:376\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams):\n\u001b[1;32m    372\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;124;03m    Select entry points from self that match the\u001b[39;00m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;124;03m    given parameters (typically group and/or name).\u001b[39;00m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 376\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m EntryPoints(ep \u001b[38;5;28;01mfor\u001b[39;00m ep \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ep\u001b[38;5;241m.\u001b[39mmatches(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams))\n",
      "File \u001b[0;32m~/micromamba/envs/diversity/lib/python3.11/importlib/metadata/__init__.py:253\u001b[0m, in \u001b[0;36mEntryPoint.matches\u001b[0;34m(self, **params)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;124;03mEntryPoint matches the given parameters.\u001b[39;00m\n\u001b[1;32m    235\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;124;03mTrue\u001b[39;00m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    252\u001b[0m attrs \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, param) \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m params)\n\u001b[0;32m--> 253\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28mmap\u001b[39m(operator\u001b[38;5;241m.\u001b[39meq, params\u001b[38;5;241m.\u001b[39mvalues(), attrs))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "calc_cr_nds_over_personas(persona70_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "322a6afe-11de-4517-8861-a807e8fbe4c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [01:00<00:00,  6.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CR: 2.7 ± 0.01\n",
      "CR-POS: 5.37 ± 0.03\n",
      "NDS: 2.84 ± 0.01\n",
      "Self-rep:2.53 ± 0.11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "crs = []\n",
    "ndss = []\n",
    "crs_pos = []\n",
    "sreps = []\n",
    "newpc = persona70_df.set_index(['persona_id', 'prompt_id'])\n",
    "for _ in tqdm(range(10)):\n",
    "    # Get random personas paired with every prompt\n",
    "    persona_ids_shuffled = [i for i in range(100)]\n",
    "    shuffle(persona_ids_shuffled)\n",
    "    prompt_ids = [i for i in range(100)]\n",
    "    pairs = list(zip(persona_ids_shuffled, prompt_ids))\n",
    "    responses = newpc.loc[pairs, 'response'].values.tolist()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    cr, cr_pos, nds, srep = calc_cr_nds_sr(responses)\n",
    "\n",
    "    crs.append(cr)\n",
    "    ndss.append(nds)\n",
    "    crs_pos.append(cr_pos)\n",
    "    sreps.append(srep)\n",
    "\n",
    "print(f\"CR: {np.round(np.mean(crs),2)} ± {np.round(np.std(crs),2)}\\nCR-POS: {np.round(np.mean(crs_pos),2)} ± {np.round(np.std(crs_pos),2)}\\nNDS: {np.round(np.mean(ndss),2)} ± {np.round(np.std(ndss),2)}\\nSelf-rep:{np.round(np.mean(sreps),2)} ± {np.round(np.std(sreps),2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cfa8da-ab2d-477b-983e-4383dd5418bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "calc_hom_over_personas(persona70_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b01e9d7-652d-42e0-8f20-368e9230d71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "persona70_df['lang'] = persona70_df.response.progress_apply(lambda x: model.predict(x.replace('\\n', ' '))[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3118a152-5c23-455d-8d99-2a2d0d8158d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in persona70_df[persona70_df.lang!='__label__eng_Latn']['persona_id'].unique():\n",
    "    print(p,personas[p])\n",
    "\n",
    "print(persona70_df[persona70_df.lang!='__label__eng_Latn']['persona_id'].unique().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add91f5e-8509-421e-884c-021068d9aa03",
   "metadata": {},
   "outputs": [],
   "source": [
    "noneng_counts=Counter(persona70_df[persona70_df.lang!='__label__eng_Latn']['persona_id'].values.tolist())\n",
    "print(noneng_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eaa7177-82d4-4ac8-ba0b-193e36c88ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_noneng_personas = [x for x in noneng_counts if noneng_counts[x]>1]\n",
    "print(possible_noneng_personas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86c4d78-50fc-42e3-9a7d-d2512db11a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "persona70_df[persona70_df.lang!='__label__eng_Latn']['prompt_id'].unique().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef573be-5188-4dd1-a7a5-7e76bdcb90dc",
   "metadata": {},
   "source": [
    "First just try all prompt responses labelled as english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d6f703-93db-49fc-a0c8-59726f3e69a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_persona70_df = persona70_df[persona70_df.lang=='__label__eng_Latn']\n",
    "print(eng_persona70_df.shape)\n",
    "calc_cr_nds_over_personas(eng_persona70_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c71ae91-868f-49e0-95d4-209c883984d8",
   "metadata": {},
   "source": [
    "Now to try with all possibly non-english personas removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b383185-62fd-4b36-9429-52abb6d8f538",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_persona70_df2 = persona70_df.loc[~persona70_df.persona_id.isin(possible_noneng_personas)]\n",
    "print(eng_persona70_df2.shape)\n",
    "calc_cr_nds_over_personas(eng_persona70_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92b114f5-efc7-4303-be3c-224401712dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What if every prompt was answered by a different personas?\n",
    "def calc_cr_nds_sr(responses):\n",
    "    cr = compression_ratio(responses, 'gzip')\n",
    "    nds = ngram_diversity_score(responses, 4)\n",
    "    #CR-POS\n",
    "    joined_pos, tuples = get_pos(responses)\n",
    "    # ngrams_pos = token_patterns(joined_pos, 5, 10)\n",
    "    cr_pos = compression_ratio(joined_pos, 'gzip')\n",
    "    srep = self_repetition_score(responses, verbose=False)\n",
    "    return cr, cr_pos, nds, srep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0dcc82b7-50a6-4c2a-8190-054a9c1bba6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:54<00:00,  5.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CR: 2.51 ± 0.02\n",
      "CR-POS: 5.04 ± 0.03\n",
      "NDS: 3.08 ± 0.02\n",
      "Self-rep:0.69 ± 0.14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "crs = []\n",
    "ndss = []\n",
    "crs_pos = []\n",
    "sreps = []\n",
    "newpc = personac70_df.set_index(['persona_id', 'prompt_id'])\n",
    "for _ in tqdm(range(10)):\n",
    "    # Get random personas paired with every prompt\n",
    "    persona_ids_shuffled = [i for i in range(100)]\n",
    "    shuffle(persona_ids_shuffled)\n",
    "    prompt_ids = [i for i in range(100)]\n",
    "    pairs = list(zip(persona_ids_shuffled, prompt_ids))\n",
    "    responses = newpc.loc[pairs, 'response'].values.tolist()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    cr, cr_pos, nds, srep = calc_cr_nds_sr(responses)\n",
    "\n",
    "    crs.append(cr)\n",
    "    ndss.append(nds)\n",
    "    crs_pos.append(cr_pos)\n",
    "    sreps.append(srep)\n",
    "\n",
    "print(f\"CR: {np.round(np.mean(crs),2)} ± {np.round(np.std(crs),2)}\\nCR-POS: {np.round(np.mean(crs_pos),2)} ± {np.round(np.std(crs_pos),2)}\\nNDS: {np.round(np.mean(ndss),2)} ± {np.round(np.std(ndss),2)}\\nSelf-rep:{np.round(np.mean(sreps),2)} ± {np.round(np.std(sreps),2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a60c846-48e8-49f7-bb15-2951c9039ba3",
   "metadata": {},
   "source": [
    "# Llama-70b temp=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "275ea540-2fef-486a-9e05-7e525a440999",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:11<00:00, 11.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CR: 2.77 ± 0.0\n",
      "CR-POS: 5.65 ± 0.0\n",
      "NDS: 2.87 ± 0.0\n",
      "Self-rep:5.79 ± 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# No persona, no cutoff\n",
    "np70_df = pd.read_csv('../output/llama-temp1/llama70b-np/Llama-3.3-70B-Instruct-Turbo_dolly_output.tsv', sep='\\t')\n",
    "np70_df['response'] = np70_df.response.apply(lambda x: x.strip())\n",
    "np70_df['prompt_id'] = [i for i in range(len(prompts))]\n",
    "np70_df['persona_id'] = [-1 for i in range(len(prompts))]\n",
    "calc_cr_nds_over_personas(np70_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7da486f-38e1-43fc-9676-eb8363f00166",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CR: 2.57 ± 0.0\n",
      "CR-POS: 5.13 ± 0.0\n",
      "NDS: 3.08 ± 0.0\n",
      "Self-rep:4.36 ± 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# No persona with cutoff\n",
    "npc70_df = pd.read_csv('../output/llama-temp1/llama70b-cutoff-np/Llama-3.3-70B-Instruct-Turbo_dolly_output.tsv', sep='\\t')\n",
    "npc70_df['response'] = npc70_df.response.apply(lambda x: x.strip())\n",
    "npc70_df['prompt_id'] = [i for i in range(len(prompts))]\n",
    "npc70_df['persona_id'] = [-1 for i in range(len(prompts))]\n",
    "calc_cr_nds_over_personas(npc70_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c04afc3e-d40d-4232-9dfc-9fbfcc866cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [06:00<00:00,  3.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CR: 2.74 ± 0.1\n",
      "CR-POS: 5.22 ± 0.16\n",
      "NDS: 2.88 ± 0.08\n",
      "Self-rep:4.55 ± 0.16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Persona plus cutoff\n",
    "personac70_df = pd.read_csv('../output/llama-temp1/llama70b-cutoff-persona/Llama-3.3-70B-Instruct-Turbo_dolly_output.tsv', sep='\\t')\n",
    "personac70_df['response'] = personac70_df.response.apply(lambda x: x.strip())\n",
    "\n",
    "if 'persona_id' not in personac70_df.columns:\n",
    "    print(\"Adding persona prompt ids\")\n",
    "    persona_ids = []\n",
    "    prompt_ids = []\n",
    "    for persona_id in range(100):\n",
    "        for prompt_id in range(100):\n",
    "            persona_ids += [persona_id]\n",
    "            prompt_ids += [prompt_id]\n",
    "    personac70_df['prompt_id'] = prompt_ids\n",
    "    personac70_df['prompt'] = personac70_df.prompt_id.apply(lambda x: prompts[x])\n",
    "    personac70_df['persona_id'] = persona_ids\n",
    "    personac70_df['persona'] = personac70_df.persona_id.apply(lambda x: personas[x])\n",
    "\n",
    "calc_cr_nds_over_personas(personac70_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8f57d0-d8aa-450f-b1f8-d9b673c9c20c",
   "metadata": {},
   "source": [
    "# Deepseek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e33c6006-75f5-4157-8514-75a739b719c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:12<00:00, 12.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CR: 2.57 ± 0.0\n",
      "CR-POS: 5.56 ± 0.0\n",
      "NDS: 3.02 ± 0.0\n",
      "Self-rep:5.75 ± 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# No persona, no cutoff\n",
    "deep_np_df = pd.read_csv('../output/deepseek-np/DeepSeek-V3_dolly_output.tsv', sep='\\t')\n",
    "deep_np_df['response'] = deep_np_df.response.apply(lambda x: x.strip())\n",
    "deep_np_df['prompt_id'] = [i for i in range(len(prompts))]\n",
    "deep_np_df['persona_id'] = [-1 for i in range(len(prompts))]\n",
    "calc_cr_nds_over_personas(deep_np_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3b2c3af-8edc-48ca-ae0d-da763ebd63d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:05<00:00,  5.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CR: 2.45 ± 0.0\n",
      "CR-POS: 5.04 ± 0.0\n",
      "NDS: 3.22 ± 0.0\n",
      "Self-rep:4.81 ± 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# No persona, plus cutoff\n",
    "deep_npc_df = pd.read_csv('../output/deepseek-np-cutoff/DeepSeek-V3_dolly_output.tsv', sep='\\t')\n",
    "deep_npc_df['response'] = deep_npc_df.response.apply(lambda x: x.strip())\n",
    "deep_npc_df['prompt_id'] = [i for i in range(len(prompts))]\n",
    "deep_npc_df['persona_id'] = [-1 for i in range(len(prompts))]\n",
    "calc_cr_nds_over_personas(deep_npc_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe81fdd-d379-44a2-8803-12bc093b7599",
   "metadata": {},
   "source": [
    "# Response length distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229356e6-d896-487f-94d7-ea479aa1c1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "len_df = {'len': [], 'source':[]}\n",
    "\n",
    "# Load all the human responses first.\n",
    "len_df['len'] += sample['response'].apply(lambda x: len(x)).values.tolist()\n",
    "len_df['source'] += ['dolly' for _ in range(len(sample))]\n",
    "\n",
    "for (df, source_name) in [(np_df, 'No persona'), (npc_df,'No persona+cutoff'), (persona_df, 'Persona'), (personac_df, 'Persona+cutoff')]:\n",
    "    len_df['len'] += df['response'].apply(lambda x: len(x)).values.tolist()\n",
    "    len_df['source'] += [source_name for _ in range(len(df))]\n",
    "    \n",
    "len_df=pd.DataFrame(len_df)\n",
    "\n",
    "g = sns.kdeplot(len_df, x='len', hue='source', common_norm=False, fill=True, clip=[-1000,6000])\n",
    "g.set_xlabel('Completion length (chars)')\n",
    "g.set_ylabel('Density')\n",
    "g.spines[\"top\"].set_visible(False)\n",
    "g.spines[\"right\"].set_visible(False)\n",
    "g.spines[\"left\"].set_visible(False)\n",
    "g.set(yticklabels=[])\n",
    "g.grid(axis='x')\n",
    "# plt.legend([], [], frameon=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4919a1df-3c0c-4339-a231-dbc480f00ff8",
   "metadata": {},
   "source": [
    "# llama-1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46b939e-235d-4d3c-ab6d-565c08920f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "np1_df = pd.read_csv('../output/llama1b-np/Llama-3.2-1B-Instruct_dolly_output.tsv', sep='\\t')\n",
    "np1_df['response'] = np1_df.response.apply(lambda x: x.strip())\n",
    "np1_df['prompt_id'] = [i for i in range(len(prompts))]\n",
    "np1_df['persona_id'] = [-1 for i in range(len(prompts))]\n",
    "calc_cr_nds_over_personas(np1_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342f83d2-f5c1-41e7-a00e-0e8503905b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "npc1_df = pd.read_csv('../output/llama1b-cutoff-np/Llama-3.2-1B-Instruct_dolly_output.tsv', sep='\\t')\n",
    "npc1_df['response'] = npc1_df.response.apply(lambda x: x.strip())\n",
    "npc1_df['prompt_id'] = [i for i in range(len(prompts))]\n",
    "npc1_df['persona_id'] = [-1 for i in range(len(prompts))]\n",
    "calc_cr_nds_over_personas(npc1_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d9a9d8-58cf-487e-8689-e888b4e41340",
   "metadata": {},
   "outputs": [],
   "source": [
    "persona1_df = pd.read_csv('../output/llama1b-persona/Llama-3.2-1B-Instruct_dolly_output.tsv', sep='\\t')\n",
    "persona1_df['response'] = persona1_df.response.apply(lambda x: x.strip())\n",
    "\n",
    "persona_ids = []\n",
    "prompt_ids = []\n",
    "for persona_id in range(100):\n",
    "    for prompt_id in range(100):\n",
    "        persona_ids += [persona_id]\n",
    "        prompt_ids += [prompt_id]\n",
    "persona1_df['prompt_id'] = prompt_ids\n",
    "persona1_df['persona_id'] = persona_ids\n",
    "calc_cr_nds_over_personas(persona1_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30619e20-dd4d-4d78-8dfa-72d89b5aa215",
   "metadata": {},
   "outputs": [],
   "source": [
    "persona1_df['lang'] = persona1_df.response.progress_apply(lambda x: model.predict(x.replace('\\n', ' '))[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded610bb-6a54-4b50-a470-dc7294a3e6f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for p in persona1_df[persona1_df.lang!='__label__eng_Latn']['persona_id'].unique():\n",
    "    print(p,personas[p])\n",
    "\n",
    "print(persona1_df[persona1_df.lang!='__label__eng_Latn']['persona_id'].unique().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e739f8-54c0-4a4a-b76a-6f4f2ef97e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "noneng_counts=Counter(persona1_df[persona1_df.lang!='__label__eng_Latn']['persona_id'].values.tolist())\n",
    "print(noneng_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2a8064-9533-486d-a935-d8fccca95732",
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_noneng_personas = [x for x in noneng_counts if noneng_counts[x]>2]\n",
    "print(possible_noneng_personas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acec87d6-f12a-4c30-9b58-cf99026f0829",
   "metadata": {},
   "outputs": [],
   "source": [
    "persona1_df[persona1_df.lang!='__label__eng_Latn']['prompt_id'].unique().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5a9a44-e8bb-4920-b92d-d8a8da025cc5",
   "metadata": {},
   "source": [
    "First just try all prompt responses labelled as english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc47082-3701-4e7a-bba6-c3e6b5154823",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_persona1_df = persona1_df[persona1_df.lang=='__label__eng_Latn']\n",
    "print(eng_persona1_df.shape)\n",
    "calc_cr_nds_over_personas(eng_persona1_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cfb16f-5c70-4841-a66c-68bc47c16cfc",
   "metadata": {},
   "source": [
    "Now to try with all possibly non-english personas removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36ec655-2573-4238-941a-3f4065305374",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_persona1_df2 = persona1_df.loc[~persona1_df.persona_id.isin(possible_noneng_personas)]\n",
    "print(eng_persona1_df2.shape)\n",
    "calc_cr_nds_over_personas(eng_persona1_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a768cf-b137-4714-84cb-94ee825199c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c694a8a-d5cc-4407-b0a3-84e0c8863937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persona plus cut off\n",
    "personac1_df = pd.read_csv('../output/llama1b-cutoff-persona/Llama-3.2-1B-Instruct_dolly_output.tsv', sep='\\t')\n",
    "personac1_df['response'] = personac1_df.response.apply(lambda x: x.strip())\n",
    "\n",
    "persona_ids = []\n",
    "prompt_ids = []\n",
    "for persona_id in range(100):\n",
    "    for prompt_id in range(100):\n",
    "        persona_ids += [persona_id]\n",
    "        prompt_ids += [prompt_id]\n",
    "personac1_df['prompt_id'] = prompt_ids\n",
    "personac1_df['persona_id'] = persona_ids\n",
    "\n",
    "calc_cr_nds_over_personas(personac1_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6e1b5e-cee6-4142-afb3-a230e6a7f686",
   "metadata": {},
   "outputs": [],
   "source": [
    "personac1_df['lang'] = personac1_df.response.progress_apply(lambda x: model.predict(x.replace('\\n', ' '))[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79de243-878d-46e9-b85c-b4072f845123",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for p in personac1_df[personac1_df.lang!='__label__eng_Latn']['persona_id'].unique():\n",
    "    print(p,personas[p])\n",
    "\n",
    "print(personac1_df[personac1_df.lang!='__label__eng_Latn']['persona_id'].unique().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d8d144-a1fa-464f-9c66-d639f1161854",
   "metadata": {},
   "outputs": [],
   "source": [
    "noneng_counts=Counter(personac1_df[personac1_df.lang!='__label__eng_Latn']['persona_id'].values.tolist())\n",
    "print(noneng_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755ff88c-bfba-475c-9289-93aa3cf73abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_noneng_personas = [x for x in noneng_counts if noneng_counts[x]>2]\n",
    "print(possible_noneng_personas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7769e7a-15f7-4638-adc3-c27aa13ffbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "personac1_df[personac1_df.lang!='__label__eng_Latn']['prompt_id'].unique().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2222da32-3223-458d-9b3b-4f779b75b5cf",
   "metadata": {},
   "source": [
    "First just try all prompt responses labelled as english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6146f75-c7de-4d41-8909-79a5043aea04",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_personac1_df = personac1_df[personac1_df.lang=='__label__eng_Latn']\n",
    "print(eng_personac1_df.shape)\n",
    "calc_cr_nds_over_personas(eng_personac1_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c361f3c2-5eb2-46a1-b84a-0d8161e2a8dc",
   "metadata": {},
   "source": [
    "Now to try with all possibly non-english personas removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8748f799-e6eb-4fdd-8765-f184ad3b2aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_personac1_df2 = personac1_df.loc[~personac1_df.persona_id.isin(possible_noneng_personas)]\n",
    "print(eng_personac1_df2.shape)\n",
    "calc_cr_nds_over_personas(eng_personac1_df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76115249-51de-4eeb-a663-7d4c7b800fbe",
   "metadata": {},
   "source": [
    "## Llama70B coarse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bd5b5bcb-c5ef-493a-beaa-d60b5316b876",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 3)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Persona plus cutoff plus coarse personas\n",
    "persona70_df_coarse = pd.read_csv('../output/coarse/llama-cutoff-persona/Llama-3.3-70B-Instruct-Turbo_dolly_output.tsv', sep='\\t')\n",
    "persona70_df_coarse['response'] = persona70_df_coarse.response.apply(lambda x: x.strip())\n",
    "\n",
    "persona70_df_coarse = persona70_df_coarse.drop_duplicates(subset=['prompt_id', 'persona_id'], keep='first')\n",
    "persona70_df_coarse.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6f16d130-2fae-440c-9ecc-6dd38d960a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:59<00:00,  5.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CR: 2.52 ± 0.02\n",
      "CR-POS: 5.07 ± 0.03\n",
      "NDS: 3.08 ± 0.02\n",
      "Self-rep:0.62 ± 0.07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "crs = []\n",
    "ndss = []\n",
    "crs_pos = []\n",
    "sreps = []\n",
    "newpc = persona70_df_coarse.set_index(['persona_id', 'prompt_id'])\n",
    "for _ in tqdm(range(10)):\n",
    "    # Get random personas paired with every prompt\n",
    "    persona_ids_shuffled = [i for i in range(100)]\n",
    "    shuffle(persona_ids_shuffled)\n",
    "    prompt_ids = [i for i in range(100)]\n",
    "    pairs = list(zip(persona_ids_shuffled, prompt_ids))\n",
    "    responses = newpc.loc[pairs, 'response'].values.tolist()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    cr, cr_pos, nds, srep = calc_cr_nds_sr(responses)\n",
    "\n",
    "    crs.append(cr)\n",
    "    ndss.append(nds)\n",
    "    crs_pos.append(cr_pos)\n",
    "    sreps.append(srep)\n",
    "\n",
    "print(f\"CR: {np.round(np.mean(crs),2)} ± {np.round(np.std(crs),2)}\\nCR-POS: {np.round(np.mean(crs_pos),2)} ± {np.round(np.std(crs_pos),2)}\\nNDS: {np.round(np.mean(ndss),2)} ± {np.round(np.std(ndss),2)}\\nSelf-rep:{np.round(np.mean(sreps),2)} ± {np.round(np.std(sreps),2)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
