{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "76ccd819-a164-4766-873d-5235d675a066",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import ipdb\n",
    "import re\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "from diversity import compression_ratio, homogenization_score, ngram_diversity_score, extract_patterns, get_pos, pos_patterns, token_patterns, self_repetition_score\n",
    "import json\n",
    "from collections import Counter\n",
    "from random import shuffle, randint\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# import mplcursors\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "sns.set(style='darkgrid', context='notebook', rc={'figure.figsize':(14,10)}, font_scale=2)\n",
    "\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('chained_assignment',None)\n",
    "\n",
    "# Set random seeds for reproducibility on a specific machine\n",
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "np.random.RandomState(1)\n",
    "np.set_printoptions(precision=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972bcea6-c7a2-48a3-8490-b20ae30a1e83",
   "metadata": {},
   "source": [
    "## Dolly human written responses diversity analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "823907fb-bc10-49b2-a061-2b0f3558c1c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a Spanish-speaking patient with severe myopia interested in LASIK eye surgery'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../data/sample_personas.txt', 'r') as f:\n",
    "    personas = [x.strip() for x in f.readlines()]\n",
    "personas[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a090ecd-41e0-4465-86a7-6f7a81b4c484",
   "metadata": {},
   "outputs": [],
   "source": [
    "dolly = load_dataset(\"databricks/databricks-dolly-15k\")[\"train\"].filter(lambda row: row['category']=='creative_writing').to_pandas()\n",
    "sample = pd.read_csv('../data/dolly_creative_prompts_sample.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2f2128c-c995-4e2f-947a-9d079bdb52bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>instruction</th>\n",
       "      <th>num_tokens</th>\n",
       "      <th>num_tokens_round</th>\n",
       "      <th>num_words_round</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>525</td>\n",
       "      <td>Please propose an argument to convince my mother that she increases the amount of money that I get every month during my studies. I think the current amount is too low.</td>\n",
       "      <td>211</td>\n",
       "      <td>210</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>172</td>\n",
       "      <td>Write a paragraph to refute a claim by a colleague that ancient structures such as Stonehenge, the Great Pyramid are evidence of UFO activities on Earth</td>\n",
       "      <td>189</td>\n",
       "      <td>190</td>\n",
       "      <td>160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>109</td>\n",
       "      <td>What is the best way to answer an interview question?</td>\n",
       "      <td>128</td>\n",
       "      <td>130</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37</td>\n",
       "      <td>Write  the first paragraph of an advertising brochure describing an hotel nearby the Annecy Lake in France</td>\n",
       "      <td>89</td>\n",
       "      <td>90</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>295</td>\n",
       "      <td>Write an intro to a meetup about music, medicine, and machines</td>\n",
       "      <td>313</td>\n",
       "      <td>310</td>\n",
       "      <td>220</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  \\\n",
       "0    525   \n",
       "1    172   \n",
       "2    109   \n",
       "3     37   \n",
       "4    295   \n",
       "\n",
       "                                                                                                                                                                instruction  \\\n",
       "0  Please propose an argument to convince my mother that she increases the amount of money that I get every month during my studies. I think the current amount is too low.   \n",
       "1                  Write a paragraph to refute a claim by a colleague that ancient structures such as Stonehenge, the Great Pyramid are evidence of UFO activities on Earth   \n",
       "2                                                                                                                     What is the best way to answer an interview question?   \n",
       "3                                                                Write  the first paragraph of an advertising brochure describing an hotel nearby the Annecy Lake in France   \n",
       "4                                                                                                            Write an intro to a meetup about music, medicine, and machines   \n",
       "\n",
       "   num_tokens  num_tokens_round  num_words_round  \n",
       "0         211               210              180  \n",
       "1         189               190              160  \n",
       "2         128               130              100  \n",
       "3          89                90               60  \n",
       "4         313               310              220  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "300fd574-c589-45c8-bed1-aa1d2461feda",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>instruction</th>\n",
       "      <th>num_tokens</th>\n",
       "      <th>num_tokens_round</th>\n",
       "      <th>num_words_round</th>\n",
       "      <th>response</th>\n",
       "      <th>prompt_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>525</td>\n",
       "      <td>Please propose an argument to convince my mother that she increases the amount of money that I get every month during my studies. I think the current amount is too low.</td>\n",
       "      <td>211</td>\n",
       "      <td>210</td>\n",
       "      <td>180</td>\n",
       "      <td>Dear Mum, I would like to talk to you about the monthly money I get for my studies. I am very grateful that you support me financially during my studies. At the same time, I feel that the current amount is a little bit too low in order to sustain myself. I estimated my monthly expenses and it seems like the money is not enough. We can go through the details if you want. I want to ask you if you can increase the amount so that I can cover my expenses. I understand if you cannot do this or have other reasons against this. However, I want you to know that if you do not increase my monthly money, I will be forced to work part-time next to my studies. There is research that shows that students who work part-time have worse grades than people who do not have to work part-time to afford their studies. I would love to dedicate my full time to my studies to get the best grades and learning outcomes possible; thus, I would prefer not to work part-time. What do you think?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>172</td>\n",
       "      <td>Write a paragraph to refute a claim by a colleague that ancient structures such as Stonehenge, the Great Pyramid are evidence of UFO activities on Earth</td>\n",
       "      <td>189</td>\n",
       "      <td>190</td>\n",
       "      <td>160</td>\n",
       "      <td>There are several strong arguments against concluding that alien races built the structures like the Stonehenge or the Pyramids. First, there is evidence that these magnificent structures were built by humans. For example, tombs of the pyramid builders were found in the vicinity of the Great Pyramind (with human remains!). The technology to move and assemble the massive stones of the Stonehenge has been shown to be available to humans at that time. Second, given that space is vast the chance of finding one tiny planet among billions is infinitesimal. If alien intelligence had indeed found our one inhabited planet they are unlikely to have just created couple of structures and avoided any further contact with Earth and its inhabitants. In science, Occam’s Razor (“the best explanation is the simplest one”) is a guiding principle and until there is much more evidence of alien visitation it would be inadvisable to jump to a conclusion that aliens built these structures.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>109</td>\n",
       "      <td>What is the best way to answer an interview question?</td>\n",
       "      <td>128</td>\n",
       "      <td>130</td>\n",
       "      <td>100</td>\n",
       "      <td>The first recommended step is to ask clarifying questions to ensure you understand the question correctly and gather the requirements needed to answer effectively. If it is a mathematical or computer science question, go ahead and work your way through the problem while you verbalize your thought process along the way. If it is a situation question, you should use the STAR method. 1. Situation - set the scene and provide context, 2. Task - describe what was needed to be done and why, 3. Action - explain what action you took, and 4. Result - describe the result of your actions and what you learned from the experience</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37</td>\n",
       "      <td>Write  the first paragraph of an advertising brochure describing an hotel nearby the Annecy Lake in France</td>\n",
       "      <td>89</td>\n",
       "      <td>90</td>\n",
       "      <td>60</td>\n",
       "      <td>This fantastic hotel is ideally located, minutes away from the Annecy Lake, one of the purest in Europe. The water temperature reaches 27 degrees celcius at the peak season, allowing the guests to swim and enjoy several water activities including wake surf and pedalo. Finally, this region is known for being the home of several cheese specialties like \"Raclette\" and \"Tartiflette\" to name a few.</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>295</td>\n",
       "      <td>Write an intro to a meetup about music, medicine, and machines</td>\n",
       "      <td>313</td>\n",
       "      <td>310</td>\n",
       "      <td>220</td>\n",
       "      <td>Music, Medicine, and Machines\\n\\nJoin us for an imaginative, authentic, and offbeat networking experience that will advance your knowledge of the technologies that impact the health and well-being of our human condition.  To make things interesting (but keep things real), our meetup incorporates an element of music to remind us, first and foremost, we want to advance technology for the greater good, but we don’t want to become robots ourselves.  Instead, we believe that “music is medicine for the soul” so our events will use a dose of music to unite us and to keep things fun and grounded in our humanity.  We’ll explore hot and emerging technologies such as:\\n\\nMachine Learning and all things AI\\nComputer and Machine Vision\\nTelemedicine\\nGenomics\\nAR/VR/MR \\nRobotics\\nCloud \\nDevOps, CI/CD, and Robotic Process Automation (RPA)\\nInfrastructure as Code (IaC) \\nChatbots\\nWearable Tech\\n3D Printing\\nBlockchain\\nAnd many more\\n\\nWe’ll talk about how these disruptive technologies improve Health &amp; Life Sciences and discuss the tenuous balance of innovation + opportunities vs privacy, security, open data, regulations, etc.  We’ll network and get to know each other to explore how each of us can get involved to ensure “the machines” benefit the communities we serve. We encourage attendees such as developers, clinicians, researchers, industry experts, students, educators, industry analysts, regulators, investors, startups, musicians, and all those willing to contribute meaningfully to our mission.</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  \\\n",
       "0    525   \n",
       "1    172   \n",
       "2    109   \n",
       "3     37   \n",
       "4    295   \n",
       "\n",
       "                                                                                                                                                                instruction  \\\n",
       "0  Please propose an argument to convince my mother that she increases the amount of money that I get every month during my studies. I think the current amount is too low.   \n",
       "1                  Write a paragraph to refute a claim by a colleague that ancient structures such as Stonehenge, the Great Pyramid are evidence of UFO activities on Earth   \n",
       "2                                                                                                                     What is the best way to answer an interview question?   \n",
       "3                                                                Write  the first paragraph of an advertising brochure describing an hotel nearby the Annecy Lake in France   \n",
       "4                                                                                                            Write an intro to a meetup about music, medicine, and machines   \n",
       "\n",
       "   num_tokens  num_tokens_round  num_words_round  \\\n",
       "0         211               210              180   \n",
       "1         189               190              160   \n",
       "2         128               130              100   \n",
       "3          89                90               60   \n",
       "4         313               310              220   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  response  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Dear Mum, I would like to talk to you about the monthly money I get for my studies. I am very grateful that you support me financially during my studies. At the same time, I feel that the current amount is a little bit too low in order to sustain myself. I estimated my monthly expenses and it seems like the money is not enough. We can go through the details if you want. I want to ask you if you can increase the amount so that I can cover my expenses. I understand if you cannot do this or have other reasons against this. However, I want you to know that if you do not increase my monthly money, I will be forced to work part-time next to my studies. There is research that shows that students who work part-time have worse grades than people who do not have to work part-time to afford their studies. I would love to dedicate my full time to my studies to get the best grades and learning outcomes possible; thus, I would prefer not to work part-time. What do you think?   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     There are several strong arguments against concluding that alien races built the structures like the Stonehenge or the Pyramids. First, there is evidence that these magnificent structures were built by humans. For example, tombs of the pyramid builders were found in the vicinity of the Great Pyramind (with human remains!). The technology to move and assemble the massive stones of the Stonehenge has been shown to be available to humans at that time. Second, given that space is vast the chance of finding one tiny planet among billions is infinitesimal. If alien intelligence had indeed found our one inhabited planet they are unlikely to have just created couple of structures and avoided any further contact with Earth and its inhabitants. In science, Occam’s Razor (“the best explanation is the simplest one”) is a guiding principle and until there is much more evidence of alien visitation it would be inadvisable to jump to a conclusion that aliens built these structures.   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          The first recommended step is to ask clarifying questions to ensure you understand the question correctly and gather the requirements needed to answer effectively. If it is a mathematical or computer science question, go ahead and work your way through the problem while you verbalize your thought process along the way. If it is a situation question, you should use the STAR method. 1. Situation - set the scene and provide context, 2. Task - describe what was needed to be done and why, 3. Action - explain what action you took, and 4. Result - describe the result of your actions and what you learned from the experience   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             This fantastic hotel is ideally located, minutes away from the Annecy Lake, one of the purest in Europe. The water temperature reaches 27 degrees celcius at the peak season, allowing the guests to swim and enjoy several water activities including wake surf and pedalo. Finally, this region is known for being the home of several cheese specialties like \"Raclette\" and \"Tartiflette\" to name a few.   \n",
       "4  Music, Medicine, and Machines\\n\\nJoin us for an imaginative, authentic, and offbeat networking experience that will advance your knowledge of the technologies that impact the health and well-being of our human condition.  To make things interesting (but keep things real), our meetup incorporates an element of music to remind us, first and foremost, we want to advance technology for the greater good, but we don’t want to become robots ourselves.  Instead, we believe that “music is medicine for the soul” so our events will use a dose of music to unite us and to keep things fun and grounded in our humanity.  We’ll explore hot and emerging technologies such as:\\n\\nMachine Learning and all things AI\\nComputer and Machine Vision\\nTelemedicine\\nGenomics\\nAR/VR/MR \\nRobotics\\nCloud \\nDevOps, CI/CD, and Robotic Process Automation (RPA)\\nInfrastructure as Code (IaC) \\nChatbots\\nWearable Tech\\n3D Printing\\nBlockchain\\nAnd many more\\n\\nWe’ll talk about how these disruptive technologies improve Health & Life Sciences and discuss the tenuous balance of innovation + opportunities vs privacy, security, open data, regulations, etc.  We’ll network and get to know each other to explore how each of us can get involved to ensure “the machines” benefit the communities we serve. We encourage attendees such as developers, clinicians, researchers, industry experts, students, educators, industry analysts, regulators, investors, startups, musicians, and all those willing to contribute meaningfully to our mission.   \n",
       "\n",
       "   prompt_id  \n",
       "0          0  \n",
       "1          1  \n",
       "2          2  \n",
       "3          3  \n",
       "4          4  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample['response'] = sample['index'].apply(lambda x: dolly.loc[x, 'response'])\n",
    "sample['prompt_id'] = [i for i in range(len(sample))]\n",
    "sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf0d1931-70b6-403b-a8fc-1be1e06701e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = sample['instruction'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb59f56a-59cd-49f4-bcac-a57a48c10f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_responses = sample['response'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "796a2ae7-609b-4cb1-9532-54be158ada3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating self-repetition score: 100%|███████████████████████████████████████████████████████| 100/100 [00:00<00:00, 86213.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CR: 2.51\n",
      "NDS: 3.03\n",
      "CR-POS: 4.93\n",
      "Self-rep: 0.55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cr = compression_ratio(human_responses, 'gzip')\n",
    "nds = ngram_diversity_score(human_responses, 4)\n",
    "joined_pos, tuples = get_pos(human_responses)\n",
    "# ngrams_pos = token_patterns(joined_pos, 5, 10)\n",
    "cr_pos = compression_ratio(joined_pos, 'gzip')\n",
    "\n",
    "srep = self_repetition_score(human_responses, verbose=True)\n",
    "\n",
    "print(f\"CR: {np.round(cr,2)}\\nNDS: {np.round(nds,2)}\\nCR-POS: {np.round(cr_pos,2)}\\nSelf-rep: {np.round(srep, 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84612ae3-e138-430c-aa63-3c7c50559f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Scoring all pairs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:15<00:00,  6.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.096\n",
      "==> Scoring all pairs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:00<00:00,  1.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "rouge = homogenization_score(human_responses, 'rougel', verbose=True)\n",
    "print(rouge)\n",
    "bleu = homogenization_score(human_responses, 'bleu', verbose=True)\n",
    "print(bleu)\n",
    "# bertscore = homogenization_score(human_responses, 'bertscore', verbose=True)\n",
    "# print(f\"HS-RougeL: {np.round(rouge,2)}\\nself-bleu: {np.round(bleu,2)}\\nHS-bert: {np.round(bertscore,2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93703b3-a60e-4eb1-9605-59e28c4635ee",
   "metadata": {},
   "source": [
    "## Main metric calculation function (can find mean and SD over persona column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b08219b-5931-4b10-bf10-5d91e6168f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_cr_nds_over_personas(df):\n",
    "    crs = []\n",
    "    ndss = []\n",
    "    crs_pos = []\n",
    "    sreps = []\n",
    "    for persona_id in tqdm(df.persona_id.unique()):\n",
    "        responses = df.loc[df.persona_id==persona_id].drop_duplicates(subset=['prompt_id'])['response'].values.tolist()\n",
    "        cr = compression_ratio(responses, 'gzip')\n",
    "        nds = ngram_diversity_score(responses, 4)\n",
    "        #CR-POS\n",
    "        joined_pos, tuples = get_pos(responses)\n",
    "        # ngrams_pos = token_patterns(joined_pos, 5, 10)\n",
    "        cr_pos = compression_ratio(joined_pos, 'gzip')\n",
    "        srep = self_repetition_score(responses, verbose=False)\n",
    "        crs.append(cr)\n",
    "        ndss.append(nds)\n",
    "        crs_pos.append(cr_pos)\n",
    "        sreps.append(srep)\n",
    "    print(f\"CR: {np.round(np.mean(crs),2)} ± {np.round(np.std(crs),2)}\\nCR-POS: {np.round(np.mean(crs_pos),2)} ± {np.round(np.std(crs_pos),2)}\\nNDS: {np.round(np.mean(ndss),2)} ± {np.round(np.std(ndss),2)}\\nSelf-rep:{np.round(np.mean(sreps),2)} ± {np.round(np.std(sreps),2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3175a55a-4934-4d6b-8846-967e2c64e1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_hom_over_personas(df):\n",
    "    bss = []\n",
    "    bleus = []\n",
    "    rls = []\n",
    "    for persona_id in tqdm(df.persona_id.unique()):\n",
    "        responses = df.loc[df.persona_id==persona_id].drop_duplicates(subset=['prompt_id'])['response'].values.tolist()\n",
    "        \n",
    "        bleu = homogenization_score(responses, 'bleu', verbose=False)\n",
    "        bleus.append(bleu)\n",
    "\n",
    "        # bs = homogenization_score(responses, 'bertscore', verbose=False)\n",
    "        # bss.append(bs)\n",
    "\n",
    "        rl = homogenization_score(responses, 'rougel', verbose=False)\n",
    "        rls.append(rl)\n",
    "        \n",
    "    print(f\"Hom-bleu: {np.round(np.mean(bleus),2)} ± {np.round(np.std(bleus),2)}\\nHom-RL:: {np.round(np.mean(rls),2)} ± {np.round(np.std(rls),2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "275ea540-2fef-486a-9e05-7e525a440999",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:05<00:00,  5.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CR: 2.57 ± 0.0\n",
      "CR-POS: 5.7 ± 0.0\n",
      "NDS: 3.02 ± 0.0\n",
      "Self-rep:1.23 ± 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# No persona, no cutoff\n",
    "npnc = pd.read_csv('../output/deepseek-np/DeepSeek-V3_dolly_output.tsv', sep='\\t')\n",
    "npnc['response'] = npnc.response.apply(lambda x: x.strip())\n",
    "npnc['prompt_id'] = [i for i in range(len(prompts))]\n",
    "npnc['persona_id'] = [-1 for i in range(len(prompts))]\n",
    "calc_cr_nds_over_personas(npnc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ede8ff5-3a61-4940-8601-87bbb608d504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Scoring all pairs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:05<00:00,  1.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Scoring all pairs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:26<00:00,  3.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hom-bleu: 0.02 ± 0.0\n",
      "Hom-RL:: 0.1 ± 0.0\n"
     ]
    }
   ],
   "source": [
    "calc_hom_over_personas(npnc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a7da486f-38e1-43fc-9676-eb8363f00166",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:06<00:00,  6.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CR: 2.45 ± 0.0\n",
      "CR-POS: 5.08 ± 0.0\n",
      "NDS: 3.22 ± 0.0\n",
      "Self-rep:0.27 ± 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# No persona with cutoff\n",
    "npc = pd.read_csv('../output/deepseek-np-cutoff/DeepSeek-V3_dolly_output.tsv', sep='\\t')\n",
    "npc['response'] = npc.response.apply(lambda x: x.strip())\n",
    "npc['prompt_id'] = [i for i in range(len(prompts))]\n",
    "npc['persona_id'] = [-1 for i in range(len(prompts))]\n",
    "calc_cr_nds_over_personas(npc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c0a4840e-157f-41ed-bd68-e9daff346829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Scoring all pairs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:00<00:00,  1.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Scoring all pairs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:12<00:00,  7.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hom-bleu: 0.0 ± 0.0\n",
      "Hom-RL:: 0.08 ± 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "calc_hom_over_personas(npc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c04afc3e-d40d-4232-9dfc-9fbfcc866cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [09:17<00:00,  5.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CR: 2.56 ± 0.08\n",
      "CR-POS: 5.04 ± 0.24\n",
      "NDS: 3.09 ± 0.06\n",
      "Self-rep:1.8 ± 0.72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Persona plus cutoff\n",
    "pc = pd.read_csv('../output/deepseek-cutoff-persona/DeepSeek-V3_dolly_output.tsv', sep='\\t')\n",
    "pc['response'] = pc.response.apply(lambda x: x.strip())\n",
    "pc = pc.drop_duplicates(subset=['prompt_id', 'persona_id'])\n",
    "calc_cr_nds_over_personas(pc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b89f80-f02e-4da2-b801-712810ce8380",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8c9cf5e0-1d41-4c7e-98c4-2b58989c6f32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process SpawnPoolWorker-759:                                                                      | 2/100 [02:24<1:57:30, 71.94s/it]\n",
      "Process SpawnPoolWorker-755:\n",
      "Process SpawnPoolWorker-757:\n",
      "Process SpawnPoolWorker-753:\n",
      "Process SpawnPoolWorker-754:\n",
      "Process SpawnPoolWorker-758:\n",
      "Process SpawnPoolWorker-756:\n",
      "Process SpawnPoolWorker-752:                                                                      | 2/100 [02:29<2:02:17, 74.87s/it]\n",
      "Process SpawnPoolWorker-751:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "                    ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "                    ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "                    ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/multiprocessing/pool.py\", line 48, in mapstar\n",
      "    return list(map(*args))\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/multiprocessing/pool.py\", line 48, in mapstar\n",
      "    return list(map(*args))\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "                    ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/multiprocessing/pool.py\", line 48, in mapstar\n",
      "    return list(map(*args))\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/site-packages/diversity/homogenization.py\", line 64, in wrapped_score\n",
      "    return _calculate_score(*args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/site-packages/diversity/homogenization.py\", line 64, in wrapped_score\n",
      "    return _calculate_score(*args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/site-packages/diversity/utils/memoize.py\", line 23, in __call__\n",
      "    value = self.func(*args)\n",
      "            ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/site-packages/diversity/homogenization.py\", line 82, in _calculate_score\n",
      "    score = scorer.compute(predictions=[pair[0]],\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/site-packages/diversity/homogenization.py\", line 64, in wrapped_score\n",
      "    return _calculate_score(*args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/multiprocessing/pool.py\", line 48, in mapstar\n",
      "    return list(map(*args))\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "                    ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args,\n",
      " **kwds))\n",
      "                    ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/multiprocessing/pool.py\", line 48, in mapstar\n",
      "    return list(map(*args))\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/site-packages/diversity/homogenization.py\", line 64, in wrapped_score\n",
      "    return _calculate_score(*args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/site-packages/diversity/utils/memoize.py\", line 23, in __call__\n",
      "    value = self.func(*args)\n",
      "            ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/site-packages/diversity/homogenization.py\", line 64, in wrapped_score\n",
      "    return _calculate_score(*args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/site-packages/diversity/utils/memoize.py\", line 23, in __call__\n",
      "    value = self.func(*args)\n",
      "            ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/site-packages/diversity/utils/memoize.py\", line 23, in __call__\n",
      "    value = self.func(*args)\n",
      "            ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/multiprocessing/pool.py\", line 48, in mapstar\n",
      "    return list(map(*args))\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/site-packages/diversity/homogenization.py\", line 82, in _calculate_score\n",
      "    score = scorer.compute(predictions=[pair[0]],\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/site-packages/evaluate/module.py\", line 455, in compute\n",
      "    self.add_batch(**inputs)\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/site-packages/diversity/homogenization.py\", line 64, in wrapped_score\n",
      "    return _calculate_score(*args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/site-packages/diversity/homogenization.py\", line 82, in _calculate_score\n",
      "    score = scorer.compute(predictions=[pair[0]],\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/site-packages/diversity/utils/memoize.py\", line 23, in __call__\n",
      "    value = self.func(*args)\n",
      "            ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/site-packages/diversity/homogenization.py\", line 82, in _calculate_score\n",
      "    score = scorer.compute(predictions=[pair[0]],\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/site-packages/diversity/utils/memoize.py\", line 23, in __call__\n",
      "    value = self.func(*args)\n",
      "            ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/site-packages/evaluate/module.py\", line 455, in compute\n",
      "    self.add_batch(**inputs)\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/site-packages/evaluate/module.py\", line 455, in compute\n",
      "    self.add_batch(**inputs)\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/site-packages/evaluate/module.py\", line 515, in add_batch\n",
      "    self._init_writer()\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/site-packages/evaluate/module.py\", line 515, in add_batch\n",
      "    self._init_writer()\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/site-packages/evaluate/module.py\", line 649, in _init_writer\n",
      "    cache_file_name, filelock = self._create_cache_file()  # get ready\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/site-packages/evaluate/module.py\", line 649, in _init_writer\n",
      "    cache_file_name, filelock = self._create_cache_file()  # get ready\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/site-packages/evaluate/module.py\", line 284, in _create_cache_file\n",
      "    filelock.acquire(timeout=timeout)\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/site-packages/evaluate/module.py\", line 515, in add_batch\n",
      "    self._init_writer()\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/site-packages/evaluate/module.py\", line 284, in _create_cache_file\n",
      "    filelock.acquire(timeout=timeout)\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/site-packages/diversity/homogenization.py\", line 82, in _calculate_score\n",
      "    score = scorer.compute(predictions=[pair[0]],\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/site-packages/filelock/_api.py\", line 344, in acquire\n",
      "    time.sleep(poll_interval)\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/site-packages/diversity/homogenization.py\", line 82, in _calculate_score\n",
      "    score = scorer.compute(predictions=[pair[0]],\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/site-packages/evaluate/module.py\", line 455, in compute\n",
      "    self.add_batch(**inputs)\n",
      "KeyboardInterrupt\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/site-packages/evaluate/module.py\", line 515, in add_batch\n",
      "    self._init_writer()\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/site-packages/evaluate/module.py\", line 649, in _init_writer\n",
      "    cache_file_name, filelock = self._create_cache_file()  # get ready\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/site-packages/filelock/_api.py\", line 344, in acquire\n",
      "    time.sleep(poll_interval)\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/site-packages/evaluate/module.py\", line 284, in _create_cache_file\n",
      "    filelock.acquire(timeout=timeout)\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "                    ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/site-packages/filelock/_api.py\", line 344, in acquire\n",
      "    time.sleep(poll_interval)\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/site-packages/evaluate/module.py\", line 455, in compute\n",
      "    self.add_batch(**inputs)\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/multiprocessing/pool.py\", line 48, in mapstar\n",
      "    return list(map(*args))\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/site-packages/evaluate/module.py\", line 515, in add_batch\n",
      "    self._init_writer()\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/site-packages/diversity/homogenization.py\", line 64, in wrapped_score\n",
      "    return _calculate_score(*args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/site-packages/evaluate/module.py\", line 649, in _init_writer\n",
      "    cache_file_name, filelock = self._create_cache_file()  # get ready\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/site-packages/evaluate/module.py\", line 455, in compute\n",
      "    self.add_batch(**inputs)\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/site-packages/diversity/utils/memoize.py\", line 23, in __call__\n",
      "    value = self.func(*args)\n",
      "            ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/site-packages/evaluate/module.py\", line 284, in _create_cache_file\n",
      "    filelock.acquire(timeout=timeout)\n",
      "KeyboardInterrupt\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/site-packages/evaluate/module.py\", line 649, in _init_writer\n",
      "    cache_file_name, filelock = self._create_cache_file()  # get ready\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/site-packages/filelock/_api.py\", line 344, in acquire\n",
      "    time.sleep(poll_interval)\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/site-packages/evaluate/module.py\", line 284, in _create_cache_file\n",
      "    filelock.acquire(timeout=timeout)\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "                    ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/site-packages/filelock/_api.py\", line 344, in acquire\n",
      "    time.sleep(poll_interval)\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/multiprocessing/pool.py\", line 48, in mapstar\n",
      "    return list(map(*args))\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/site-packages/diversity/homogenization.py\", line 82, in _calculate_score\n",
      "    score = scorer.compute(predictions=[pair[0]],\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/site-packages/diversity/homogenization.py\", line 64, in wrapped_score\n",
      "    return _calculate_score(*args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/site-packages/diversity/utils/memoize.py\", line 23, in __call__\n",
      "    value = self.func(*args)\n",
      "            ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/site-packages/diversity/homogenization.py\", line 82, in _calculate_score\n",
      "    score = scorer.compute(predictions=[pair[0]],\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/site-packages/evaluate/module.py\", line 467, in compute\n",
      "    output = self._compute(**inputs, **compute_kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "  File \"/Users/venkat/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--bleu/9e0985c1200e367cce45605ce0ecb5ede079894e0f24f54613fca08eeb8aff76/bleu.py\", line 122, in _compute\n",
      "    score = compute_bleu(\n",
      "            ^^^^^^^^^^^^^\n",
      "  File \"/Users/venkat/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--bleu/9e0985c1200e367cce45605ce0ecb5ede079894e0f24f54613fca08eeb8aff76/nmt_bleu.py\", line 76, in compute_bleu\n",
      "    translation_ngram_counts = _get_ngrams(translation, max_order)\n",
      "                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/site-packages/evaluate/module.py\", line 515, in add_batch\n",
      "    self._init_writer()\n",
      "  File \"/Users/venkat/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--bleu/9e0985c1200e367cce45605ce0ecb5ede079894e0f24f54613fca08eeb8aff76/nmt_bleu.py\", line 44, in _get_ngrams\n",
      "    ngram_counts[ngram] += 1\n",
      "    ~~~~~~~~~~~~^^^^^^^\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/site-packages/evaluate/module.py\", line 455, in compute\n",
      "    self.add_batch(**inputs)\n",
      "KeyboardInterrupt\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/site-packages/evaluate/module.py\", line 515, in add_batch\n",
      "    self._init_writer()\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/site-packages/evaluate/module.py\", line 649, in _init_writer\n",
      "    cache_file_name, filelock = self._create_cache_file()  # get ready\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/site-packages/evaluate/module.py\", line 284, in _create_cache_file\n",
      "    filelock.acquire(timeout=timeout)\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/site-packages/filelock/_api.py\", line 344, in acquire\n",
      "    time.sleep(poll_interval)\n",
      "KeyboardInterrupt\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/site-packages/evaluate/module.py\", line 649, in _init_writer\n",
      "    cache_file_name, filelock = self._create_cache_file()  # get ready\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/site-packages/evaluate/module.py\", line 284, in _create_cache_file\n",
      "    filelock.acquire(timeout=timeout)\n",
      "KeyboardInterrupt\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/site-packages/filelock/_api.py\", line 344, in acquire\n",
      "    time.sleep(poll_interval)\n",
      "Keyboard"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m calc_hom_over_personas(pc)\n",
      "Cell \u001b[0;32mIn[20], line 8\u001b[0m, in \u001b[0;36mcalc_hom_over_personas\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m persona_id \u001b[38;5;129;01min\u001b[39;00m tqdm(df\u001b[38;5;241m.\u001b[39mpersona_id\u001b[38;5;241m.\u001b[39munique()):\n\u001b[1;32m      6\u001b[0m     responses \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mloc[df\u001b[38;5;241m.\u001b[39mpersona_id\u001b[38;5;241m==\u001b[39mpersona_id]\u001b[38;5;241m.\u001b[39mdrop_duplicates(subset\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprompt_id\u001b[39m\u001b[38;5;124m'\u001b[39m])[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m----> 8\u001b[0m     bleu \u001b[38;5;241m=\u001b[39m homogenization_score(responses, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbleu\u001b[39m\u001b[38;5;124m'\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      9\u001b[0m     bleus\u001b[38;5;241m.\u001b[39mappend(bleu)\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# bs = homogenization_score(responses, 'bertscore', verbose=False)\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# bss.append(bs)\u001b[39;00m\n",
      "File \u001b[0;32m~/micromamba/envs/diversity/lib/python3.11/site-packages/diversity/homogenization.py:49\u001b[0m, in \u001b[0;36mhomogenization_score\u001b[0;34m(data, measure, use_stemmer, model, verbose)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ind1, curr_str \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28menumerate\u001b[39m(data), total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(data), disable\u001b[38;5;241m=\u001b[39m(\u001b[38;5;129;01mnot\u001b[39;00m verbose)):\n\u001b[1;32m     48\u001b[0m     all_args \u001b[38;5;241m=\u001b[39m [((curr_str, data[ind2]), scorer, measure, model) \u001b[38;5;28;01mfor\u001b[39;00m ind2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(data)) \u001b[38;5;28;01mif\u001b[39;00m ind2\u001b[38;5;241m!=\u001b[39mind1] \n\u001b[0;32m---> 49\u001b[0m     doc_scores \u001b[38;5;241m=\u001b[39m pool\u001b[38;5;241m.\u001b[39mmap(wrapped_score, all_args)\n\u001b[1;32m     50\u001b[0m     corpus_score \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(doc_scores)\u001b[38;5;241m/\u001b[39m(\u001b[38;5;28mlen\u001b[39m(data)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# case where all strings are the exact same in the list\u001b[39;00m\n",
      "File \u001b[0;32m~/micromamba/envs/diversity/lib/python3.11/multiprocessing/pool.py:367\u001b[0m, in \u001b[0;36mPool.map\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmap\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, iterable, chunksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    363\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;124;03m    Apply `func` to each element in `iterable`, collecting the results\u001b[39;00m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;124;03m    in a list that is returned.\u001b[39;00m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m--> 367\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_map_async(func, iterable, mapstar, chunksize)\u001b[38;5;241m.\u001b[39mget()\n",
      "File \u001b[0;32m~/micromamba/envs/diversity/lib/python3.11/multiprocessing/pool.py:768\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    769\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mready():\n\u001b[1;32m    770\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m\n",
      "File \u001b[0;32m~/micromamba/envs/diversity/lib/python3.11/multiprocessing/pool.py:765\u001b[0m, in \u001b[0;36mApplyResult.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwait\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 765\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event\u001b[38;5;241m.\u001b[39mwait(timeout)\n",
      "File \u001b[0;32m~/micromamba/envs/diversity/lib/python3.11/threading.py:622\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    620\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    621\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 622\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cond\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    623\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m~/micromamba/envs/diversity/lib/python3.11/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         waiter\u001b[38;5;241m.\u001b[39macquire()\n\u001b[1;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Interrupt\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "                    ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/multiprocessing/pool.py\", line 48, in mapstar\n",
      "    return list(map(*args))\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/site-packages/diversity/homogenization.py\", line 64, in wrapped_score\n",
      "    return _calculate_score(*args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/site-packages/diversity/utils/memoize.py\", line 23, in __call__\n",
      "    value = self.func(*args)\n",
      "            ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/site-packages/diversity/homogenization.py\", line 82, in _calculate_score\n",
      "    score = scorer.compute(predictions=[pair[0]],\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/site-packages/evaluate/module.py\", line 455, in compute\n",
      "    self.add_batch(**inputs)\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/site-packages/evaluate/module.py\", line 515, in add_batch\n",
      "    self._init_writer()\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/site-packages/evaluate/module.py\", line 649, in _init_writer\n",
      "    cache_file_name, filelock = self._create_cache_file()  # get ready\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/site-packages/evaluate/module.py\", line 284, in _create_cache_file\n",
      "    filelock.acquire(timeout=timeout)\n",
      "  File \"/Users/venkat/micromamba/envs/diversity/lib/python3.11/site-packages/filelock/_api.py\", line 344, in acquire\n",
      "    time.sleep(poll_interval)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "calc_hom_over_personas(pc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2749a586-01a5-46df-a132-76896c293725",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9996, 3)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pc.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d049d08-9de9-439f-90e6-31897fecce5a",
   "metadata": {},
   "source": [
    "## Coarse persona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bb606851-7ed0-4639-9afa-032b430f29a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [09:08<00:00,  5.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CR: 2.65 ± 0.06\n",
      "CR-POS: 5.18 ± 0.15\n",
      "NDS: 2.96 ± 0.06\n",
      "Self-rep:2.27 ± 0.72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Persona plus cutoff\n",
    "coarse_pc70 = pd.read_csv('../output/coarse/llama-cutoff-persona/Llama-3.3-70B-Instruct-Turbo_dolly_output.tsv', sep='\\t')\n",
    "coarse_pc70['response'] = coarse_pc70.response.apply(lambda x: x.strip())\n",
    "coarse_pc70 = coarse_pc70.drop_duplicates(subset=['prompt_id', 'persona_id'])\n",
    "calc_cr_nds_over_personas(coarse_pc70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "86406ebc-d2d7-4f78-87bc-0e16db9b16ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 3)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coarse_pc70[coarse_pc70.response.str.len()<8].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c090497-b267-4f05-89d9-111ef8931713",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12',\n",
       "       '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23',\n",
       "       '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34',\n",
       "       '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45',\n",
       "       '46', '47', 'persona_id', '48', '49', '50', '51', '52', '53', '54',\n",
       "       '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65',\n",
       "       '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76',\n",
       "       '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87',\n",
       "       '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98',\n",
       "       '99'], dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coarse_pc70.persona_id.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c612e5-b1ea-4267-9f2c-224961c10c87",
   "metadata": {},
   "source": [
    "## What if we calculate the metrics with each prompt answered by a different persona?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "840231cd-4eb4-4728-a071-c71fdc944c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_cr_nds_over_personas(df):\n",
    "    crs = []\n",
    "    ndss = []\n",
    "    crs_pos = []\n",
    "    sreps = []\n",
    "    for persona_id in tqdm(df.persona_id.unique()):\n",
    "        responses = df.loc[df.persona_id==persona_id].drop_duplicates(subset=['prompt_id'])['response'].values.tolist()\n",
    "        cr = compression_ratio(responses, 'gzip')\n",
    "        nds = ngram_diversity_score(responses, 4)\n",
    "        #CR-POS\n",
    "        joined_pos, tuples = get_pos(responses)\n",
    "        # ngrams_pos = token_patterns(joined_pos, 5, 10)\n",
    "        cr_pos = compression_ratio(joined_pos, 'gzip')\n",
    "        srep = self_repetition_score(responses, verbose=False)\n",
    "        crs.append(cr)\n",
    "        ndss.append(nds)\n",
    "        crs_pos.append(cr_pos)\n",
    "        sreps.append(srep)\n",
    "    print(f\"CR: {np.round(np.mean(crs),2)} ± {np.round(np.std(crs),2)}\\nCR-POS: {np.round(np.mean(crs_pos),2)} ± {np.round(np.std(crs_pos),2)}\\nNDS: {np.round(np.mean(ndss),2)} ± {np.round(np.std(ndss),2)}\\nSelf-rep:{np.round(np.mean(sreps),2)} ± {np.round(np.std(sreps),2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21c3da31-9137-4b89-b8ae-eb399eb05557",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9996, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pc = pd.read_csv('../output/deepseek-cutoff-persona/DeepSeek-V3_dolly_output.tsv', sep='\\t')\n",
    "pc['response'] = pc.response.apply(lambda x: x.strip())\n",
    "pc = pc.drop_duplicates(subset=['prompt_id', 'persona_id'])\n",
    "pc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b00886c6-9d39-4e84-89d1-7a91629822d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "response    \"Mom, I really appreciate everything you’re doing to support my studies—it means so much to me. I want to propose that we revisit the amount you’re sending me each month because, with rising costs, it’s becoming challenging to manage everything. I’m fully committed to my studies and my future, and I know this investment will pay off. I’ve been tracking my expenses carefully, and things like textbooks, software, and even basic living costs have gone up significantly. I’m not asking for more than necessary—just enough to ensure I can focus on my education without unnecessary stress. \\n\\nThink of it this way: by increasing my budget slightly, you’re helping me avoid distractions like taking on additional part-time work that could take away from my studies. I’m dedicated to making the most of this opportunity and giving myself the best shot at a secure and successful career. Your support now is an investment in my future, and I’m committed to making you proud. Let’s work together to figure out a reasonable adjustment that works for both of us.\"\n",
       "Name: (40, 0), dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newpc = pc.set_index(['persona_id', 'prompt_id'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f676f50c-2036-4b75-bcd3-12aead3ed2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_cr_nds_sr(responses):\n",
    "    cr = compression_ratio(responses, 'gzip')\n",
    "    nds = ngram_diversity_score(responses, 4)\n",
    "    #CR-POS\n",
    "    joined_pos, tuples = get_pos(responses)\n",
    "    # ngrams_pos = token_patterns(joined_pos, 5, 10)\n",
    "    cr_pos = compression_ratio(joined_pos, 'gzip')\n",
    "    srep = self_repetition_score(responses, verbose=False)\n",
    "    return cr, cr_pos, nds, srep\n",
    "    # print(f\"CR: {np.round(np.mean(cr),2)} \\nCR-POS: {np.round(np.mean(cr_pos),2)}\\nNDS: {np.round(np.mean(nds),2)}\\nSelf-rep: {np.round(np.mean(srep),2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2f7f23cb-d964-4323-879d-be604248dc7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:53<00:00,  5.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CR: 2.42 ± 0.01\n",
      "CR-POS: 4.95 ± 0.04\n",
      "NDS: 3.21 ± 0.02\n",
      "Self-rep:0.32 ± 0.06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# What if every prompt was answered by a different persona?\n",
    "crs = []\n",
    "ndss = []\n",
    "crs_pos = []\n",
    "sreps = []\n",
    "\n",
    "for _ in tqdm(range(10)):\n",
    "    # Get random personas paired with every prompt\n",
    "    persona_ids_shuffled = [i for i in range(100)]\n",
    "    shuffle(persona_ids_shuffled)\n",
    "    prompt_ids = [i for i in range(100)]\n",
    "    pairs = list(zip(persona_ids_shuffled, prompt_ids))\n",
    "    responses = newpc.loc[pairs, 'response'].values.tolist()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    cr, cr_pos, nds, srep = calc_cr_nds_sr(responses)\n",
    "\n",
    "    crs.append(cr)\n",
    "    ndss.append(nds)\n",
    "    crs_pos.append(cr_pos)\n",
    "    sreps.append(srep)\n",
    "\n",
    "print(f\"CR: {np.round(np.mean(crs),2)} ± {np.round(np.std(crs),2)}\\nCR-POS: {np.round(np.mean(crs_pos),2)} ± {np.round(np.std(crs_pos),2)}\\nNDS: {np.round(np.mean(ndss),2)} ± {np.round(np.std(ndss),2)}\\nSelf-rep:{np.round(np.mean(sreps),2)} ± {np.round(np.std(sreps),2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "548f045f-bba0-4651-bda7-c2405f6b8f5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:53<00:00,  5.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CR: 2.45 ± 0.04\n",
      "CR-POS: 4.99 ± 0.08\n",
      "NDS: 3.18 ± 0.02\n",
      "Self-rep:0.83 ± 0.22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# What if there are 10 different personas that answered all 100 prompts?\n",
    "\n",
    "crs = []\n",
    "ndss = []\n",
    "crs_pos = []\n",
    "sreps = []\n",
    "\n",
    "for _ in tqdm(range(10)):\n",
    "    # Get random personas paired with every prompt\n",
    "    persona_ids_shuffled = [randint(0,99) for _ in range(10)]*10\n",
    "    shuffle(persona_ids_shuffled)\n",
    "    prompt_ids = [i for i in range(100)]\n",
    "    pairs = list(zip(persona_ids_shuffled, prompt_ids))\n",
    "    responses = newpc.loc[pairs, 'response'].values.tolist()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    cr, cr_pos, nds, srep = calc_cr_nds_sr(responses)\n",
    "\n",
    "    crs.append(cr)\n",
    "    ndss.append(nds)\n",
    "    crs_pos.append(cr_pos)\n",
    "    sreps.append(srep)\n",
    "\n",
    "print(f\"CR: {np.round(np.mean(crs),2)} ± {np.round(np.std(crs),2)}\\nCR-POS: {np.round(np.mean(crs_pos),2)} ± {np.round(np.std(crs_pos),2)}\\nNDS: {np.round(np.mean(ndss),2)} ± {np.round(np.std(ndss),2)}\\nSelf-rep:{np.round(np.mean(sreps),2)} ± {np.round(np.std(sreps),2)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2a1d68-a7a1-4e7a-8bb4-c3a24f218740",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
